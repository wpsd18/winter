# Vecteurs gaussiens

## Objectifs 

 * Définition des vecteurs gaussiens et énoncé des propriétés de base. 
 * Comprendre comment l'information peut être résumée dans la matrice de covariance. 
 * Visualiser l'information portée par les vecteurs propres (axes principaux) de cette matrice dans un problème à deux classes. 
 * Premier neurone logique. 


## Définition des lois gaussiennes

Soit ${\bf C}$ une matrice de covariance de dimension $D \times D$ et ${\bf m}$ un vecteur de dimension $D$. La loi gaussienne $N({\bf m},{\bf C})$ est la loi de densité suivante

$$
p({\bf x}) = \frac{1}{(2\pi)^{D/2} (\det{\bf C})^{1/2}} 
\exp(-\frac12 ({\bf x} - {\bf m})^T {\bf C}^{-1} ({\bf x}- {\bf m}) ) \, , \quad {\bf x} \in \mathbb{R}^D.
$$

## Propriétés

### Espérance et matrice de covariance.   

L'espérance de la loi $N({\bf m},{\bf C})$  est égale à ${\bf m}$ et la matrice de covariance est égale à ${\bf C}$

$$
\mathbb{E}[{\bf x}] = {\bf m} \, , \quad {\rm cov}[{\bf x}] = {\bf C}. 
$$
Nous admettons provisoirement ce résultat. Une démonstration sera donnée plus loin. 

### Lois marginales. 

Les lois marginales des coordonnées du vecteur ${\bf x} = (x_1, \dots, x_D)^T$ sont les lois normales de moyenne $m_j$ et de variance $c_{jj}$

$$
p_{\sf x_j}(x_j) = N(x_j | m_j, \sigma^2_j = c_{jj}) \, , \quad x_j \in \mathbb{R},
$$

où  $m_{j}$ représente la $j$ème coordonnée du vecteur ${\bf m}$ et $c_{jj}$ représente le $j$ème terme  diagonal de la matrice ${\bf C}$.


Par exemple, la densité de la loi $N({\bf m = 0},{\bf C})$ de moyenne nulle et de matrice de covariance 

$$
{\bf C} = \left(\begin{array}{cc}  2 & 1 \\ 1 & 1  \end{array}\right) \, ,
$$
est donnée par 

$$
p(x_1, x_2) = \frac1{2\pi} \exp(- x_1^2/2 + x_1 x_2 - x_2^2)  \, , \quad (x_1, x_2) \in \mathbb{R}^2.
$$
Les lois marginales des variables $x_1$ et $x_2$ sont respectivement les lois normales $N(0,2)$ et $N(0,1)$.

### Caractérisation spectrale

Dans cette section, nous proposons une définition de la loi gaussienne s'appuyant directement sur la matrice de covariance. Nous considérons tout d'abord des vecteurs d'espérance nulle. Une définition de la loi normale équivalente à la définition donnée précédemment est la suivante (admis). 

**Définition.** On dira qu'un vecteur ${\bf x}$ de $\mathbb{R}^D$ suit une loi normale de moyenne nulle s'il existe $D$ scalaires strictement positifs, $(\lambda_j)$, $j = 1, \dots, D$, et une matrice unitaire ${\bf U}$ de dimension $D\times D$ tels que les coordonnées $z_j$ du vecteur 

$$
{\bf z} = {\bf U}^T {\bf x}
$$
sont indépendantes et de loi normale $N(0, \lambda_j)$, pour tout $j = 1, \dots, D$. Une matrice unitaire est telle que ${\bf U}{\bf U}^T={\bf I}$. 


Notons ${\bf \Lambda}$ la matrice diagonale dont les termes correspondent aux valeurs $(\lambda_j)$

$$
{\bf \Lambda} = \left( \begin{array}{ccc}
\lambda_1 & 0 & 0\\
 0 &.  & 0\\
 0 & 0  & \lambda_D\\
\end{array}
\right) .
$$

Par indépendance des coordonnées, la matrice de covariance du vecteur ${\bf z}$ est diagonale, égale à ${\bf \Lambda}$, et nous avons 

$$
{\bf \Lambda} = {\bf cov[z]} = {\bf U}^T {\bf cov[x]} {\bf U} .
$$

Cela implique que la matrice de covariance du vecteur ${\bf x}$ est donnée par l'équation suivante

$$
{\bf C} = {\bf cov[x]}  = {\bf U} {\bf \Lambda} {\bf U}^T \, . 
$$

Une conséquence immédiate de la définition précédente est que **les variances, $\lambda_j$, correspondent aux valeurs propres de la matrice de covariance du vecteur ${\bf x}$ et que la matrice ${\bf U}$ correspond aux vecteurs propres de cette matrice**. 

Pour prouver ce résultat, multiplions les termes de l'équation précédente à gauche par ${\bf U}$ et notons que ${\bf \Lambda}$ commute avec ${\bf U}$ (car elle est diagonale). Nous avons donc

$$
 {\bf C} {\bf U}  = {\bf \Lambda} {\bf U} \, .
$$
Cette équation signifie que les vecteurs colonnes de la matrice ${\bf U}$ sont bien les vecteurs propres de la matrice ${\bf C}$ associés aux valeurs propres $(\lambda_j)$. Pour tout $j = 1, \dots, D$, nous avons en effet

$$
{\bf C}  {\bf u}_j = \lambda_j {\bf u}_j \, .
$$

Dans cette définition, nous avons supposé la moyenne nulle (vecteur centré), mais ce n'est pas restrictif. En effet, pour décentrer le vecteur aléatoire ${\bf x}$, il suffit de lui ajouter un vecteur constant, ${\bf m}$, non nul. On obtient alors un vecteur gaussien de moyenne ${\bf m}$.     


### Linéarité et indépendance 

La définition précédente permet de déduire quelques propriétés très importantes des vecteurs gaussiens. Les démonstrations sont laissées comme exercice (la première propriété se démontre directement à partir des propriétés de bases des densités de probabilité et est admise).

* La **somme** de deux variables aléatoires de lois normales $N(m_1,\sigma_1^2)$ et $N(m_2,\sigma_2^2)$ **indépendantes** est une variable aléatoire de loi normale $N(m_1+m_2,\sigma_1^2+\sigma_2^2)$  (admis).   

* Toute _combinaison linéaire_ des coordonnées d'un vecteur gaussien suit _une loi normale_.

* En particulier, les lois marginales d'un vecteur gaussien sont des lois normales. 

* Soit ${\bf x} \in \mathbb{R}^D$ un vecteur gaussien  de loi $N({\bf m}, {\bf C})$. Pour toute application linéaire ${\bf A}$ de rang $K \leq D$, le vecteur ${\bf Ax}$ est gaussien de moyenne ${\bf Am}$ et de matrice de covariance ${\bf ACA}^T$.

* Soit ${\bf x} \in \mathbb{R}^D$ un vecteur gaussien  de loi $N({\bf m}, {\bf C})$. Les coordonnées $(x_i)$ sont indépendantes si et seulement si la matrice de covariance ${\bf C}$ est diagonale. 


### Lignes de niveau de la densité 

Les lignes de niveaux de la densité de la loi $N({\bf m = 0}, {\bf C})$ sont des ellipses d'équation
$$
{\bf x}^T {\bf C}^{-1} {\bf x} = c^2 \,. 
$$
Dans cette équation, nous pouvons considérer que $c^2$ est un nombre positif car les valeurs propres de la matrice ${\bf C}$ sont strictement positives. 

Pour démontrer ce résultat, nous observons que 
$$
 {\bf x}^T {\bf C}^{-1} {\bf x} = ({\bf x}^T {\bf U}) \, {\bf \Lambda }^{-1}  \, ({\bf U}^T{\bf x}) = {\bf z}^T {\bf \Lambda}^{-1} {\bf z} \, .
$$
Cette dernière expression se réécrit simplement 

$$
 {\bf z}^T {\bf \Lambda}^{-1} {\bf z} = \sum_{j=1}^D \frac{z_j^2}{\lambda_j} .
$$

Ainsi, lorsqu'elles sont exprimées dans les coordonnées transformées, ${\bf z}$, déduites des coordonnées initiales par la rotation ${\bf U}^T$, les lignes de niveau d'une densité gaussienne correspondent à des ellipses dont les axes sont parallèles aux vecteurs de base

$$
 \sum_{j=1}^D \frac{z_j^2}{\lambda_j} = c^2 .
$$


Les axes des ellipses ne dépendent pas de la valeur $c$ et sont appelés les _axes principaux_ de la matrice de covariance. Les axes principaux sont parallèles aux vecteurs propres (${\bf u}_j$) de la matrice de covariance. 

En général les axes principaux sont ordonnés. Le premier axe principal correspond à la plus grande valeur propre de ${\bf C}$ tandis que le dernier correspond à la plus petite valeur propre.  Par exemple, la densité de la loi $N({\bf m = (1,1)},{\bf C})$ de matrice de covariance 

$$
{\bf C} = \left(\begin{array}{cc}  2 & 1 \\ 1 & 1  \end{array}\right) \, ,
$$
est décrite par la densité dont les lignes de niveau elliptiques et l'orientation des axes sont montrés ci-dessous.  Les points représentent un échantillon de $n = 200$ tirages de cette loi. 


```{r cm3_density_gauss_2D, echo = FALSE}
  #Code to construct plot
  mu <- c(1,1)
  sigma <- matrix(c(2,1,1,1),nrow=2)
  x <- mvtnorm::rmvnorm(200, mean=mu, sigma=sigma)
  
  x_points <- seq(min(x[,1]), max(x[,1]), length.out=100)
  y_points <- seq(min(x[,2]), max(x[,2]), length.out=100)
  
  z <- matrix(0,nrow=100,ncol=100)

    for (i in 1:100) {
      for (j in 1:100) {
    z[i,j] <- mvtnorm::dmvnorm(c(x_points[i], y_points[j]), mean=mu, sigma=sigma)
      }
    }

  plot(x_points, y_points, type = "n")
  points(x, col = "grey60", xlab = "x1",  ylab = "x2", cex = .5, pch = 19)
  
  contour(x_points, y_points, z, col = "darkblue", lwd = 2, add = T)
```
 
 
La même densité peut se représenter sous forme topographique de la manière suivante.  
 
```{r cm3_density_contour, echo = FALSE}
  image(x_points, y_points, z, col = terrain.colors(20))
  contour(x_points, y_points, z, col = "darkblue", lwd = 2, add = T)
```



## Simulation : composantes principales d'un mélange gaussien

Simuler un vecteur gaussien peut se faire de manière très simple en R, par exemple, en utlisant la fonction `rmvnorm()` de la bibliothèque `mvtnorm`. Dans de nombreux problèmes d'apprentissage supervisé, les lois gaussiennes sont utilisées comme des modèles probabilistes pour la distribution des variables au sein des catégories à apprendre. 

Pour fixer les idées, plaçons nous en dimension $D=2$. Considérons un modèle à deux classes décrit par une variable aléatoire binaire $y \in \{0,1\}$ codée en couleurs _bleu_ et _orange_. Supposons que les classes sont équiprobables 

$$ p(y = 0) = p(y = 1) = \frac12.$$

Cela revient à supposer que les deux classes sont d'effectifs voisins.

Au sein de la classe bleue (0), le vecteur ${\bf x}$ admet pour loi  $N({\bf m}_0, {\bf I})$ où ${\bf m_0}=(3,0)$, tandis qu'au sein de la classe orange (1), ce vecteur admet pour loi $N({\bf m}_1, {\bf I})$ où ${\bf m_1}=(0,3)$. Un tel modèle est appelé _modèle de mélange_. La loi du vecteur ${\bf x}$ est donnée par la formule suivante

$$
p({\bf x})  = \frac12 N({\bf x}| {\bf m}_0, { \bf I}) + \frac12 N({\bf x}| {\bf m}_1, { \bf I}), \quad {\bf x} \in \mathbb{R}^2.
$$

Une simulation permet de visualiser 200 vecteurs générés par le modèle de mélange
_
```{r cm3_mixture}
  # Simulation d'un échantillon de taille n de matrice de covariance Id
    n <- 200
    Id <- diag(2)
    
  # Tirage des classes equiprobables selon la loi binomiale
    n_blue <- rbinom(1, size = n, prob = 1/2) 
    classes <- c(rep("blue", n_blue), rep("orange", n - n_blue))
  
  # Tirages gaussiens à l'intérieur des classes  
    x_0 <- mvtnorm::rmvnorm(n_blue, mean = c(3,0), sigma = Id)
    x_1 <- mvtnorm::rmvnorm(n - n_blue, mean = c(0,3), sigma = Id)
  
  # Les observations des deux classes sont aggrégées en ligne et affichées    
    x <- rbind(x_0, x_1)
    plot(x, col = classes, pch = 19)
  
```

Dans cette simulation, la classe 0 est donc représentée en bleu, tandis que la classe 1 est représentée en orange. Le graphique permet d'envisager de séparer les deux classes par une droite. Calculons la matrice de covariance empirique et représentons les projections des $n$ vecteurs (${\bf x}_i$) sur les axes principaux. Les projections sont données par la représentation spectrale

$${\bf z}_i  =  {\bf U}^T {\bf x}_i \, \quad   i = 1, \dots, n. $$

```{r cm3_pca}
library(magrittr)

  # Calcule la decomposition spectrale de la matrice de covariance empirique
  U <- x %>% cov() %>% eigen()
  
  # Effectue les projections sur les axes principaux
  # Caveat : les vecteurs z et x sont transposés
  z <- x %*% U$vectors 
  
  # on affiche les classes et la ligne 0
  z %>% plot(col = classes, pch = 19, xlab = "z1", ylab = "z2")
  abline(v = 0, lty = 2, col = "grey")
```


Nous aboutissons à une nouvelle représentation des données obtenue par combinaison linéaire des données originales, appelée _analyse en composantes principales_. Notons que l'axe de plus grande variance (axe $z_1$) contient l'information nécessaire pour séparer les deux classes et proposer un algorithme de classication (grossièrement) supervisée par la couleur majoritaire le long de cet axe.

La projection sur le premier axe principal permet de proposer une règle très simple de classification.  Cette règle est la suivante

$$
\forall {\bf x} \in \mathbb{R}^2, \quad {\rm classe}({\bf x}) = \left\{ \begin{array}{l} 1 & {\rm si~} \quad  u_{11}x_1 + u_{21}x_2 > 0, \\ 
0 & {\rm sinon.} \\ \end{array} \right.
$$

où les valeurs $u_{11} \approx -0.69$ et $u_{21} \approx 0.72$ sont donc calculées par l'algorithme numérique de diagonalisation matricielle `eigen()`. 

Nous venons de voir un exemple très simple dans lequel on aboutit à une fonction logique appelée _neurone formel_ ou _neurone de McCulloch-Pitts_. La fonction calcule une combinaison linéaire pondérée des entrées $x_1$ et $x_2$, la compare à un seuil ($b=0$) et propose un classement d'après une décision déterministe. L'algorithme d'apprentissage consiste simplement à effectuer calcul des coefficients $u_{ij}$. Il correspond la décomposition spectrale de la matrice de covariance empirique.  



## Principales notions à retenir et à savoir définir

Dans cette séance, nous avons vu les points suivants.

* La **loi normale multimensionnelle** est caractérisée par la donnée d'une valeure moyenne et d'une matrice de covariance. Il n'est pas nécessaire de connaître par coeur la formule de la densité d'un vecteur gaussien. En revanche, il est fort utile de mémoriser la formule de la loi en dimension un. 

* Les lignes de niveau de la densité de la loi normale multimensionnelle sont des ellipses dont les axes correspondent aux axes principaux de la matrice de covariance. 

* La loi normale multimensionnelle est caractérisée par sa **représentation spectrale**.

* Les **valeurs propres de la matrice de covariance** correspondent aux variances des projections des observations sur les axes principaux. Les axes de plus grande variance sont les plus informatifs. 

* Projeter les observations sur **les axes principaux** de plus grande variance permet de visualiser l'information contenue dans les données. Elle permet aussi de proposer un algorithme permettant de regrouper les observations en classes distinctes. 



 









