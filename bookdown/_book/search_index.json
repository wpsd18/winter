[
["travaux-diriges-3.html", "Chapitre 9 Travaux dirigés 3 9.1 Objectif de la séance 9.2 Exercice 1. Perte 0-1 (accuracy) et prédiction dure. 9.3 Exercice 2. L’algorithme des \\(k\\) plus proches voisins.", " Chapitre 9 Travaux dirigés 3 9.1 Objectif de la séance On suppose que l’on effectue une classification binaire à partir du vecteur \\({\\bf x} \\in \\mathbb{R}^D\\). On note \\(y \\in \\{0,1\\}\\) la classe associée au vecteur \\({\\bf x}\\) et on appelle classifieur dur (hard classifier) toute une fonction \\(c({\\bf x})\\) retournant 0 ou 1, supposée prédire la classe du vecteur \\({\\bf x}\\). On suppose de plus que les données ont été générées en proportions égales dans les deux classes \\[ p(y = 0) = p(y = 1) = \\frac12\\,. \\] L’objectif de cette séance de travaux dirigés est de vérifier que le prédicteur optimal minimisant la fonction de perte 01 (erreur de classification) est lié aux probabilités conditionnelles d’appartenance aux classes sachant les observations \\({\\bf x}\\). Nous calculerons le classifieur optimal dans un cas gaussien, et vérifierons qu’il s’agit d’un neurone formel. Dans la seconde partie, nous étudierons les propriétés d’un classifieur non-paramétrique. 9.2 Exercice 1. Perte 0-1 (accuracy) et prédiction dure. On considère la fonction de perte 01 pénalisant un mauvais classement. Cette fonction est définie de la manière suivante \\[ \\forall {\\bf x} \\in \\mathbb{R}^D \\,, \\, y \\in \\{0,1\\} \\,, \\quad L(c({\\bf x}), y) = \\mathbb{1}_{(c({\\bf x}) \\neq y)} \\, . \\] En d’autres termes, la perte est égale à 1 si \\(c({\\bf x})\\) n’est pas égal à \\(y\\). Elle est égale à 0 sinon. 9.2.1 Question 1 Justifier que la perte moyenne correspond au taux de mauvais classement \\[ \\mathbb{E}[ L(c({\\bf x}), y)] = p(y \\neq c({\\bf x})) \\, . \\] 9.2.2 Question 2 Vérifier rapidement que, si \\(c({\\bf x}) = 1\\), nous avons \\[ p(y \\neq c({\\bf x}) | {\\bf x}) = p(y = 0 | {\\bf x}) \\, , \\] et, si \\(c({\\bf x}) = 0\\), nous avons \\[ p(y \\neq c({\\bf x}) | {\\bf x}) = p(y = 1 | {\\bf x}) \\, . \\] 9.2.3 Question 3 Montrer que la fonction \\(c_{\\rm opt}({\\bf x})\\) minimisant l’espérance de la variable \\(L(c({\\bf x}),y)\\) est égale à \\[ c_{\\rm opt}({\\bf x}) = \\left\\{ \\begin{array}{cl} 1 &amp; {\\rm si~~} p(y = 1 | {\\bf x}) &gt; p(y = 0 | {\\bf x}) \\\\ 0 &amp; {\\rm sinon.} \\\\ \\end{array} \\right. \\] On appelera la fonction \\(c_{\\rm opt}({\\bf x})\\) le classifieur optimal. La courbe de \\(\\mathbb{R}^D\\) définie par l’équation \\[ p(y = 1 | {\\bf x}) = p(y = 0 | {\\bf x}) \\] s’appelle la frontière de décision. 9.2.4 Question 4 Dans cette question, on suppose que \\(D = 1\\) et que l’observation est une variable réelle, \\(x \\in \\mathbb{R}\\). Pour chacune des deux classes, on suppose que les données ont été générées selon une loi normale de moyenne \\(m_0\\) ou \\(m_1\\) et de variance 1 de la manière suivante \\[ p( x | y = k ) = N(x | m_k, 1 ) \\, , \\quad k = 0, 1. \\] Appliquer la formule de Bayes pour montrer que la règle de classification optimale au sens de la fonction de perte 01 est donnée par la fonction suivante \\[ c_{\\rm opt}(x) = \\left\\{ \\begin{array}{cl} 1 &amp; {\\rm si~~} d(x,m_1) &lt; d(x,m_0) \\\\ 0 &amp; {\\rm sinon,} \\\\ \\end{array} \\right. \\] où \\(d(x,m)\\) désigne la distance (euclidienne) entre \\(x\\) et \\(m\\). Donner une interprétation simple de cette règle de classification en décrivant la frontière de décision. 9.2.5 Question facultative 4bis On suppose que les données n’ont pas nécessairement été générées en proportion égale dans deux classes \\[p(y = 1) \\neq p(y = 0).\\] Déterminer le classifieur optimal et l’équation de la frontière de décision. Pour cela, on pose \\(\\pi_0 = p(y = 0)\\) et \\(\\pi_1= p(y = 1)\\). 9.2.6 Question 5 : Décision neuronale Montrer que la fonction \\(c_{\\rm opt}(x)\\) de la question 4 correspond à la définition d’un neurone artificiel logique de la forme \\[ n(x) = \\left\\{ \\begin{array}{cl} 1 &amp; {\\rm si~~} wx - b &gt; 0 \\\\ 0 &amp; {\\rm sinon,} \\\\ \\end{array} \\right. \\] où \\(w = (m_1 - m_0)\\) et \\(b = w(m_0 + m_1)/2\\). Symboliser graphiquement le neurone logique ainsi défini. 9.2.7 Question 6 : Classification probabiliste - soft classifier En utilisant la formule de Bayes, montrer que la probabilité conditionnelle de \\(y = 1\\) sachant \\(x\\) est décrite par la formule suivante \\[ p(y = 1 | x) = {\\rm sigmoid}(wx - b) \\] où la fonction sigmoid\\((x)\\) est définie par \\[ {\\rm sigmoid}(x) = \\frac{1}{1 + e^{-x}} \\, , \\quad x \\in \\mathbb{R}. \\] Vérifier cette probabilité conditionnelle correspond à la définition d’un neurone artificiel probabiliste (symboliser graphiquement le neurone probabiliste ainsi défini). 9.3 Exercice 2. L’algorithme des \\(k\\) plus proches voisins. 9.3.1 Question 1 En dimension \\(D\\) quelconque, on dispose d’un échantillon d’apprentissage constitué de \\(n\\) observations vectorielles \\(({\\bf x}_i)_{i=1,\\dots,n}\\) et de \\(n\\) valeurs binaires \\((y_i)_{i=1,\\dots,n}\\), correspondant à la classification des observations. Pour un vecteur test \\({\\bf x}\\), on considère l’ensemble des \\(k\\) plus proches voisins de \\({\\bf x}\\) dans l’échantillon. Une définition formelle de \\(V({\\bf x})\\) peut être trouvée ci-dessous \\[ V({\\bf x}) = \\{ {\\bf x}_{(j)} , j = 1, \\dots, k, {\\rm ~t.q.~} \\max_{j = 1, \\dots, k } d({\\bf x}_{(j)}, {\\bf x}) &lt; \\min_{j = k+1, \\dots, n } d({\\bf x}_{(j)}, {\\bf x}) \\} \\] où \\(d\\) désigne la distance euclidienne et \\((j)\\) désigne la permutation des éléments de l’échantillon d’apprentissage ordonnant les distances à \\({\\bf x}\\). On considère l’algorithme de classification appelé \\(k\\)-NN s’appuyant sur une règle de vote local selon élection au suffrage universel direct par mode de scrutin majoritaire \\[ \\hat{c}({\\bf x}) = \\left\\{ \\begin{array}{cl} 1 &amp; {\\rm si~~} \\frac1k \\sum_{i \\in V({\\bf x})} y_i &gt; \\frac12 \\\\ 0 &amp; {\\rm sinon.} \\\\ \\end{array} \\right. \\] On suppose que l’échantillon est de taille arbitrairement grande (\\(n \\to \\infty\\)). Sous quelles conditions portant sur \\(k\\) et sur la taille (rayon) du voisinage \\(V({\\bf x})\\) pourrait-on justifier l’approximation suivante \\[ \\frac1k \\sum_{i \\in V({\\bf x})} y_i \\approx p(y = 1 | {\\bf x}) \\, ? \\] Justifier votre argument par un graphique et par la loi des grands nombres. 9.3.2 Question 2 L’algorithme de construction du classifieur \\(\\hat{c}({\\bf x})\\) implémente-t-il la règle de classification de Bayes ? Justifier la réponse. 9.3.3 Question 3 On suppose à nouveau que \\(n\\) est très grand, et on considère le classifieur probabiliste décrit par \\[ \\hat q({\\bf x}; k) = \\frac1k \\sum_{i \\in V({\\bf x})} y_i \\, . \\] Pour les cas extrêmes \\(k = 1\\) et \\(k = n\\), justifier les valeurs de l’espérance conditionnelle ci-dessous (on suppose que \\({\\bf x}\\) est indépendant de l’échantillon d’apprentissage) \\[ \\mathbb{E}[\\hat q({\\bf x}; k=1) | {\\bf x}] \\approx p(y=1|{\\bf x}) \\, , \\] \\[ \\mathbb{E}[\\hat q({\\bf x}; k=n) | {\\bf x}] = p(y=1) \\, . \\] Quel prédicteur vous paraît le plus exact ? 9.3.4 Question 4 Pour les cas extrêmes \\(k = 1\\) et \\(k = n\\), justifier les approximations de la variance de \\(\\hat q({\\bf x};k)\\) \\[ {\\rm Var}[\\hat q({\\bf x}; k=1) ] \\approx {\\rm Var}[ y ] \\, , \\] \\[ {\\rm Var}[\\hat q({\\bf x}; k=n) ] \\approx \\frac{{\\rm Var}[ y ]}{n} \\, . \\] Quel prédicteur vous paraît le plus variable ? Comment les résultats précédents peuvent-ils influencer le choix de \\(k\\) ? 9.3.5 Question 5. Application au modèle de Hastie et Tibshirani. Le modèle HT (de Hastie et Tibshirani) consiste à simuler un feu d’artifice gaussien à deux classes. La première classe est centrée en \\((1,0)\\) et \\(10\\) sous-classes sont générées selon la loi \\(N((1,0), {\\bf I})\\). Les observations sont alors simulées en choisissant une sous-classe \\(m_k\\) au hasard parmi les 10 sous-classes puis en générant des données selon une loi de variance \\(\\sigma^2\\), \\(N(m_k, \\sigma^2{\\bf I})\\). La seconde classe obéit au même modèle génératif, mais à partir du point central \\((0,1)\\). Simulons un jeu de données en utilisant le générateur rhastib(), disponible dans le package du cours, et affichons les données d’apprentissage puis les données de test colorées par classe x &lt;- rhastib(400, 400, sigma2 = 0.2) summary(x) ## Length Class Mode ## train 800 -none- numeric ## test 800 -none- numeric ## class_train 400 -none- character ## class_test 400 -none- character plot(x$train, pch = 19, col = x$class_train) plot(x$test, pch = 19, col = x$class_test) Où vous semble passer la frontière de décision ? Vous parait-elle être linéaire ? 9.3.6 Question 6 On entraîne un classifieur \\(k\\)-NN sur les données du modèle hastib tout en évaluant son erreur de classification sur un ensemble test de même taille. On effectue ceci pour \\(k = 1, \\dots, 50\\). Commenter le code suivant (4 lignes de commentaires à compléter). acc_train &lt;- NULL acc_test &lt;- NULL for (k in 1:50){ # comment 1: mod_knn &lt;- class::knn(x$train, x$train, x$class_train, k = k, prob = FALSE) # comment 2: acc_train[k] &lt;- mean(mod_knn == x$class_train) # comment 3 mod_knn &lt;- class::knn(x$train, x$test, x$class_train, k = k, prob = FALSE) # comment 4 acc_test[k] &lt;- mean(mod_knn == x$class_test) } 9.3.7 Question 7 On affiche les résultats de la manière suivante. plot(c(0,50), c(min(c(acc_train, acc_test)), max(c(acc_train, acc_test))), xlab = &quot;Number of neighbors (k)&quot;, ylab = &quot;Accurary&quot;, type = &quot;n&quot;) points(acc_train, col = &quot;blue&quot;, type = &quot;l&quot;, lwd = 3) points(acc_test, col = &quot;red&quot;, type = &quot;l&quot;, lwd = 3) legend(x = 42, y = 0.95, legend = c(&quot;train&quot;, &quot;test&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), lty = 1) Commenter la courbe obtenue. Quel choix de \\(k\\) vous apparait être justifié pour des prédictions futures concernant des tirages identiques et indépendants de ceux effectués dans la simulation ? Commenter l’effet de \\(k\\) sur ces courbes. 9.3.8 Question 8 Commenter les codes correspondant à l’experience suivante: visualiser la frontière de décision. Utiliser l’aide de R pour les commandes inconnues. #################################################################### x_coord &lt;- seq(min(x$train[,1]), max(x$train[,1]), length = 100) y_coord &lt;- seq(min(x$train[,2]), max(x$train[,2]), length = 100) ## comment 1 matrice_test &lt;- cbind(rep(x_coord, length = 100), rep(y_coord, each = 100)) ## comment 2 mod_knn &lt;- class::knn(x$train, matrice_test, x$class_train, k = 30, prob = TRUE) ## Récupère les probabilités conditionnelles des 2 classes calculées par le modèle (objet mod_knn) pred &lt;- attr(mod_knn,&quot;prob&quot;) ## comment 3 pred[mod_knn != &quot;orange&quot;] &lt;- 1 - pred[mod_knn != &quot;orange&quot;] proba &lt;- matrix(pred, nrow = 100) ## comment 4 image(x_coord, y_coord, proba, col = grey.colors(10), main = &quot;k = 30&quot;) points(x$train, col = x$class_train) A quoi correspond la carte générée dans l’image ci-dessus ? Interpréter les lignes de niveau de cette carte. ## comment image(x_coord, y_coord, matrix(pred &gt; 0.5, nrow = 100), col = grey.colors(2), main = &quot;k = 30&quot;) points(x$train, col = x$class_train) A quoi correspond la frontière de décision dans la carte ci-dessus ? Comment est elle obtenue ? ## comment 1 mod_knn &lt;- class::knn(x$train, matrice_test, x$class_train, k = 2, prob = TRUE) ## Récupère les probabilités des 2 classes calculées par le modèle (objet mod_knn) pred &lt;- attr(mod_knn,&quot;prob&quot;) ## comment 2 pred[mod_knn != &quot;orange&quot;] &lt;- 1 - pred[mod_knn != &quot;orange&quot;] ## comment 3 image(x_coord, y_coord, matrix(pred &gt; 0.5, nrow = 100), col = grey.colors(2), main = &quot;k = 2&quot;) ## comment 4 points(x$train, col = x$class_train) Commenter le résultat affiché ci-dessus. Commenter le code. "]
]
