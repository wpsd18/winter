[
["travaux-diriges-2.html", "Chapitre 7 Travaux dirigés 2 7.1 Objectif de la séance 7.2 Exercice 1. Un premier neurone artificiel. 7.3 Exercice 2. A propos des matrices de covariance.", " Chapitre 7 Travaux dirigés 2 7.1 Objectif de la séance L’objectif de cette séance de travaux dirigés est de comprendre concrètement l’utilité de la décomposition spectrale d’une matrice de covariance dans un exemple de données multidimensionelles réelles. Il s’agit aussi d’interpréter les axes principaux de cette matrice. 7.2 Exercice 1. Un premier neurone artificiel. L’objectif de cet exercice est de montrer comment l’analyse spectrale de la matrice de covariance d’un ensemble de données permet de prendre une décision concernant la provenance de ces données (classification binaire). Nous montrons que cette analyse conduit à une décision effectuée par un neurone artificiel, c’est à dire une fonction décrite de la manière suivante \\[ \\forall {\\bf x} \\in \\mathbb{R}^D, \\quad g({\\bf x}) = \\varphi( \\sum_{j = 1}^D w_{j} x_j - \\theta) = \\varphi( {\\bf w}^T {\\bf x} - \\theta) \\, , \\] où \\(\\varphi(t)\\) est une fonction de décision. Les fonctions de décision les plus courantes sont la fonction sigmoide (\\(\\varphi(t) = 1/(1 + e^{-t})\\)) et la fonction de Heavyside (\\(\\varphi(t) = 1_{\\mathbb{R}_+}(t)\\)). Neurone artificiel (source wikipedia). Nous utiliserons un jeu de données constitué de 699 échantillons de tissus provenant de tumeurs du sein, comportant neuf descripteurs des tumeurs et une classification binaire (“benign”, “malignant”). Les descripteurs seront détaillés plus précisement lors d’une séance de travaux pratiques. 7.2.1 Question 1 Nous chargeons les données et éliminons 16 lignes où apparaissent des données manquantes. library(&#39;mlbench&#39;) data(BreastCancer) # eliminer les lignes avec des données manquantes boo.na &lt;- ! apply(BreastCancer, 1, anyNA) # créer une matrice de données x &lt;- data.matrix(BreastCancer[boo.na,-c(1,11)]) type &lt;- BreastCancer[boo.na,11] dim(x) ## [1] 683 9 La matrice de données comporte neuf mesures effectuées sur 683 femmes. Nous calculons de la matrice de covariance empirique de ces mesures de la manière suivante Sigma &lt;- cov(x) Quelles sont les dimensions de la matrice de covariance ? 7.2.2 Question 2 Ecrire la décomposition spectrale de la matrice \\(\\Sigma\\). Comment interpreter la rotation \\(U\\) et les valeurs propres associées à chaque axe ? (Raisonner comme si le vecteur de mesures était gaussien.) 7.2.3 Question 3 Les valeurs propres de la matrice de covariance sont calculées de la manière suivante lambda &lt;- eigen(Sigma)$values lambda ## [1] 48.9833621 5.0826660 4.3016808 3.1485202 2.7570225 2.3036290 ## [7] 1.7827455 1.5398118 0.8068218 cumsum(lambda) # somme cumulée des valeurs propres ## [1] 48.98336 54.06603 58.36771 61.51623 64.27325 66.57688 68.35963 69.89944 ## [9] 70.70626 Quel(s) axe(s) porte(nt) le plus d’information ? Pourquoi ? 7.2.4 Question 4 Après avoir centré les mesures observées, on souhaite calculer les projections de ces mesures sur les axes propres. Vérifier que la solution donnée par le code suivant est correcte et interpréter le résultat du code. U &lt;- eigen(Sigma)$vectors m &lt;- apply(x, 2, mean) projection &lt;- (x - m) %*% U col &lt;- c(&quot;grey&quot;,&quot;orange&quot;) plot(projection, pch = 19, col = col[1 + (type == &quot;malignant&quot;)]) mean((projection[,1]&lt;0) == (type == &quot;malignant&quot;)) ## [1] 0.9751098 Note: la fonction scale(,scale = FALSE) centre les données. 7.2.5 Question 5 Proposer une méthode simple, s’appuyant sur la décomposition spectrale de la matrice \\(\\sigma\\), pour classer les types “malins” et “bénins”. Indication: Tout d’abord, on calcule les moyennes \\({\\bf m}\\) des mesures observées, puis on centre l’observation \\({\\bf x}\\). Vérifier ensuite, que pour prendre la décision, on peut calculer le produit scalaire entre \\({\\bf x - m}\\) et le premier axe de la rotation. Si la valeur est positive, le diagnostic sera bénin (1), sinon malin (0). \\[ {\\rm diagnostic} = \\left\\{ \\begin{array}{lc} 1 &amp; {\\rm si } \\sum_{j=1}^9 u_j x_j &gt; s_0 \\\\ 0 &amp; {\\rm si } \\sum_{j=1}^9 u_j x_j &lt; s_0 \\\\ \\end{array} \\right. \\] où le seuil de décision \\(s_0\\) est donné par l’équation \\[ s_0 = \\sum_{i=1}^9 u_j m_j. \\] 7.2.6 Conclusion Nous avons ainsi construit un neurone logique pour lequel les paramètres, parfois appelés coefficients synaptiques, sont “appris” à partir des données par la décomposition spectrale de la matrice de covariance. Nous pouvons voir comment ce neurone diagnostique la patiente de caractéristiques \\(x.681\\). x.681 &lt;- x[681,] m &lt;- apply(log(x), 2, mean) u.type &lt;- unique(type) x.681 ## Cl.thickness Cell.size Cell.shape Marg.adhesion ## 5 10 10 3 ## Epith.c.size Bare.nuclei Bl.cromatin Normal.nucleoli ## 7 3 8 10 ## Mitoses ## 2 pred &lt;- u.type[ 1 + ((log(x.681) - m) %*% U[,1] &lt; 0) ] #La prédiction est pred ## [1] malignant ## Levels: benign malignant 7.3 Exercice 2. A propos des matrices de covariance. L’objectif de cet exercice est de déterminer une condition pour que la matrice symétrique définie ci-dessous soit une matrice de covariance. \\[ \\Sigma = \\left( \\begin{array}{cccccc} \\sigma^2_1 &amp; 0 &amp;. &amp; .&amp; 0 &amp; c_1 \\\\ 0 &amp; \\sigma^2_2 &amp; &amp; &amp; 0 &amp; c_2 \\\\ . &amp; &amp; &amp; &amp; . &amp; .\\\\ . &amp; &amp; &amp; &amp; 0 &amp; . \\\\ 0 &amp; 0 &amp; . &amp; 0 &amp; \\sigma^2_K &amp; c_K \\\\ c_1 &amp; c_2 &amp; . &amp; .&amp; c_K &amp; \\sigma^2_{K+1} \\\\ \\end{array} \\right) . \\] Pour cela, nous proposons un processus de construction d’un vecteur gaussien \\({\\bf x}\\) de loi \\(N(0, \\Sigma)\\). 7.3.1 Question 1 Rappeler une condition nécessaire et suffisante pour que la matrice \\(\\Sigma\\) soit une matrice de covariance. 7.3.2 Question 2 : Cas particulier \\(K = 2\\). On considère une variable aléatoire \\(x_1\\) de loi \\(N(0, \\sigma^2_1)\\). On pose ensuite \\[ x_2 = \\beta_1 x_1 + \\epsilon \\] où \\(\\beta_1\\) est un scalaire et \\(\\epsilon\\) est une variable aléatoire de loi normale \\(N(0, \\sigma^2)\\), indépendante de \\(x_1\\). Montrer que \\({\\bf x}\\) est un vecteur gaussien. Déterminer la matrice de covariance de \\({\\bf x} = (x_1, x_2)^T\\). 7.3.3 Question 3 En déduire que les conditions pour que le vecteur \\({\\bf x}\\) ait pour loi \\(N(0, \\Sigma)\\) sont \\[ \\beta_1 = c_1/\\sigma_1^2 \\] et \\[ \\sigma^2 = \\sigma_2^2 (1 - \\rho^2) \\, , \\] où \\(\\rho^2 = c_1^2/(\\sigma_1^2\\sigma_2^2)\\). 7.3.4 Question 4 Retrouver ainsi la condition (nécessaire) pour que \\(\\Sigma\\) soit une matrice de covariance. 7.3.5 Question 5 : Cas général. Dans le cas général, on suppose que les variables \\(x_k\\) sont mutuellement indépendantes et de loi respectives \\(N(0, \\sigma^2_k)\\) pour \\(k = 1, \\dots, K\\). On construit la coordonnée \\(x_{K+1}\\) comme combinaison linéaire des \\(K\\) coordonnées précédentes, plus un bruit, de la manière suivante \\[ x_{K+1} = \\sum_{k=1}^K \\beta_k x_k + \\epsilon \\, . \\] où \\(\\epsilon\\) est indépendante des \\(x_k\\) et de loi \\(N(0, \\sigma^2)\\). Trouver les valeurs des coefficients \\(\\beta_k\\) et \\(\\sigma^2\\) permettant de définir un vecteur gaussien de matrice de covariance \\(\\Sigma\\). 7.3.6 Question 6 On pose \\(\\rho_k^2 = c_k^2/(\\sigma_k^2\\sigma_{K+1}^2)\\). Montrer qu’une condition suffisante pour que \\(\\Sigma\\) soit bien définie est \\[ R^2 = \\sum_{k=1}^K \\rho_k^2 &lt; 1 . \\] Commentaire : le coefficient \\(R^2\\) s’appelle le coefficient de corrélation carrée multiple. "]
]
