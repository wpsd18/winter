[
["travaux-diriges-5.html", "Chapitre 13 Travaux dirigés 5 13.1 Objectif de la séance 13.2 Exercice 1. Erreur quadratique 13.3 Exercice 2. Perte entropique ou logloss. 13.4 Exercice 3. Décomposition de l’erreur quadratique.", " Chapitre 13 Travaux dirigés 5 13.1 Objectif de la séance L’objectif de cette séance de travaux dirigés est de comprendre l’optimalité de la prédiction probabiliste (pourquoi chercher à calculer des probabilités conditionnelles). On étudiera par ailleurs l’effet de la régularisation sur les réseaux de neurones. Dans un problème de classification à deux classes (0 et 1), on appelera prédiction probabiliste (soft) l’action d’estimer ou d’approcher la probabilité qu’une donnée de l’ensemble test soit de la classe 1. Un prédicteur probabiliste sera une fonction des variables observées, \\(\\hat{q}({\\bf x})\\), telle que \\[ \\hat{q}({\\bf x}) \\approx q({\\bf x}) \\equiv p(y = 1 | {\\bf x}). \\] Les modèles de prédiction comportent souvent des paramètres, notés (\\({\\bf \\theta}\\)). La phase d’apprentissage nous permet d’optimiser ces paramètres en considérant un ensemble d’observations et les catégories de ces variables observées. On note le résultat de la phase d’apprentissage \\(\\hat{\\bf \\theta}\\). Le modèle de prédiction sera alors une fonction de \\({\\bf x}\\) où l’on a injecté ce résultat \\[ \\hat{q}({\\bf x}) \\equiv q({\\bf x}, \\hat{\\bf \\theta}). \\] La prédiction n’a de sens que pour un ensemble test indépendant de l’ensemble des observations utilisées pendant l’apprentissage. 13.2 Exercice 1. Erreur quadratique 13.2.1 Question 1 Dans un problème de classification binaire, on considère que l’on peut approcher les observations de classe 0 ou 1 par une fonction \\(\\phi({\\bf x})\\) prenant ses valeurs dans l’intervalle fermé \\([0,1]\\) (et pour laquelle des valeurs différentes de 0 ou 1 sont autorisées). On définit la fonction de perte quadratique de la façon suivante \\[ L(\\phi({\\bf x}), y) = (y - \\phi({\\bf x}))^2 \\, , \\quad y \\in \\{ 0, 1\\}, \\quad {\\bf x} \\in \\mathbb{R}^D \\, . \\] Montrer que \\[ \\mathbb{E}[L(\\phi({\\bf x}), y)| {\\bf x} ] \\geq q({\\bf x}) = p(y = 1| {\\bf x}) \\, , \\] pour toute fonction \\(\\phi({\\bf x}) \\in [0,1]\\). Indication : Ajouter et retrancher l’espérance conditionnelle \\(\\mathbb{E}[ y | {\\bf x} ]\\). Calculer cette espérance conditionnelle. 13.2.2 Question 2. Optimalité du prédicteur probabiliste. En déduire l’optimalité du prédicteur \\(q({\\bf x} ) = p(y = 1| {\\bf x})\\) au sens des moindres carrés \\[ \\mathbb{E}[ (y - \\phi({\\bf x}))^2 ] \\geq \\mathbb{E}[ (y - q({\\bf x}) )^2 ] , \\quad \\forall \\phi({\\bf x}) \\in [0,1] \\, . \\] 13.2.3 Question 3 On dispose d’un échantillon de test. Quelle formule empirique permet de calculer l’erreur de prédiction (valeur moyenne de la fonction de perte ? On donnera une réponse pour \\(\\phi({\\bf x})\\). 13.2.4 Question 4 Ecrire des instructions R permettant le calcul des erreurs pour un modèle dont on dispose d’une fonction predict(). 13.3 Exercice 2. Perte entropique ou logloss. On définit la perte entropique ou log-loss de la façon suivante \\[ L(\\phi({\\bf x}), y) = - y \\log \\phi({\\bf x}) - (1-y)\\log(1 - \\phi({\\bf x})) \\, , \\quad y = 0, 1. \\] 13.3.1 Question 1 On dispose d’un échantillon de test. Quelle formule empirique permet de calculer la valeur moyenne de la fonction de perte ? 13.3.2 Question 2 Montrer que \\[ \\mathbb{E}[ L(\\phi({\\bf x}),y) | {\\bf x} ] = - p(y = 1 | {\\bf x} ) \\log \\phi({\\bf x}) - p(y = 0 | {\\bf x} ) \\log(1 - \\phi({\\bf x})) \\, . \\] 13.3.3 Question 3 Pour une valeur de \\({\\bf x}\\) fixée, on définit la loi de probabilité \\(p_\\phi(.|{\\bf x})\\) sur \\(\\{ 0, 1\\}\\) telle que \\[ p_\\phi(1|{\\bf x}) = \\phi({\\bf x}) \\] et \\[ p_\\phi(0|{\\bf x}) = 1 - \\phi({\\bf x}) \\, . \\] Pour tout \\({\\bf x}\\), montrer que \\[ \\mathbb{E}[ L(\\phi({\\bf x}), y) | {\\bf x} ] - h(p(y = . | {\\bf x} )) = D_{\\rm KL}( p(y = . | {\\bf x} ) \\, \\| \\, p_\\phi( . | {\\bf x} ) ) \\, , \\] où \\(h(p)\\) est l’information de la loi \\(p\\) et \\(D_{\\rm KL}\\) est la divergence de Kullback-Liebler. 13.3.4 Question 4 En déduire que le prédicteur probabiliste \\(q({\\bf x}) = p(y = 1 | {\\bf x})\\) est optimal au sens du critère d’entropie \\[ \\mathbb{E}[ L(\\phi({\\bf x}) , y) ] \\geq \\mathbb{E}[ L(q({\\bf x}), y) ] \\, , \\quad \\forall \\phi({\\bf x}) \\in [0,1]. \\] 13.3.5 Question 5 Ecrire des instructions permettant le calcul des fonctions de perte pour un modèle dont on dispose d’une fonction predict(). 13.4 Exercice 3. Décomposition de l’erreur quadratique. 13.4.1 Question 1 Montrer que l’erreur quadratique se décompose de la manière suivante. \\[ \\mathbb{E}[ (\\hat{q}({\\bf x}) - q({\\bf x}))^2 ] = {\\rm var}(\\hat{q}({\\bf x})) + (\\mathbb{E}[\\hat{q}({\\bf x}) - q({\\bf x})])^2 \\, . \\] Interpréter les deux termes de l’expression de droite, et expliquer ce qui pourrait agir sur chacun des termes (taille des échantillons, nombre de paramètres, complexité du modèle de prédiction). 13.4.2 Question 2. Réseaux neuronaux. Soit \\({\\bf x} \\in \\mathbb{R}^D\\). On appelle réseau de neurones à une couche cachée un prédicteur probabiliste de la forme \\[ q({\\bf x}, \\theta) = {\\rm sigmoid}( {\\bf W_0^T} \\, {\\rm sigmoid}( {\\bf W_1^T} {\\bf x} - {\\bf b_1} ) - {\\bf b_0}) \\, . \\] Les matrices \\({\\bf W_0^T}\\) et \\({\\bf W_1^T}\\) sont de dimensions respectives \\(D \\times H\\) et \\(H \\times 1\\), où \\(H\\) est le nombre de neurones de la couche cachée. Les vecteurs \\({\\bf b_0}\\) et \\({\\bf b_1}\\) sont de dimensions respectives \\(H\\) et \\(1\\). La fonction sigmoide est appliquée à chaque coordonnée des vecteurs. Les coefficients des matrices \\({\\bf W}\\) sont appelés les poids (weights) et les coefficients \\(b\\) sont appelés les seuils (bias). Le paramètre global \\(\\theta\\) correspond à \\[ \\theta = ({\\bf W_0}, {\\bf W_1}, {\\bf b_0}, {\\bf b_1}). \\] Le paramètre global est donc de dimension \\((D + 2)H + 1\\). En dimension \\(D=2\\) et en utilisant 20 neurones, nous aurons ainsi 81 paramètres. La formule peut se généraliser à plusieurs couches. On parle alors de réseaux multi-couches. On qualifie les réseaux multi-couches de réseaux profonds à partir d’une douzaine de couches. Les réseaux de neurones apprennent en minimisant une fonction de perte pénalisée de la forme suivante \\[ L(\\theta) = \\frac1n \\sum_{i=1}^n L(q({\\bf x}_i, \\theta), y_i) + \\lambda \\| \\theta \\|_2^2 \\, , \\quad \\lambda &gt; 0. \\] L’optimisation est réalisée par un algorithme numérique (un algorithme de gradient stochastique par exemple) agissant directement sur le paramètre \\(\\theta\\). Le paramètre \\(\\lambda\\) est un paramètre régularisateur, dont le rôle est de contrebalancer une complexité trop élevée du modèle (\\(H\\) trop grand). En d’autres termes, une grande valeur de \\(\\lambda\\) forcera certains poids à être très faibles. En considérant une grande valeur de \\(\\lambda\\), discuter l’effet de ce paramètre sur la fonction de perte quadratique. Comment chacun des deux termes de l’erreur quadratique moyenne est-il sensible au changement de ce paramètre ? 13.4.3 Question 3 Commenter les codes et les résultats des expériences suivantes où l’on fait varier les valeurs de \\(H\\) et de \\(\\lambda\\) # Simulation selon de la modèle HT. 400 données sont équiréparties en 2 classes source(&quot;./codes/TP3.r&quot;) x &lt;- rhastib(200, 200, sigma2 = 0.1) # Création d&#39;une grille de valeurs tests pour les prédictions d&#39;un # modèle probabiliste #################################################################### x.coord &lt;- seq(min(x$train[,1]), max(x$train[,1]), length = 100) y.coord &lt;- seq(min(x$train[,2]), max(x$train[,2]), length = 100) matrice.test &lt;- cbind(rep(x.coord, length = 100), rep(y.coord, each = 100)) #################################################################### # comment 1 mod_nnet &lt;- nnet::nnet(x$train, x$class.train == &quot;orange&quot;, size = 50, #size = H decay = 0.0, #decay = lambda maxit = 500, trace = FALSE, entropy = TRUE) # prediction sur toute la zone d&#39;étude pred &lt;- predict(mod_nnet, matrice.test) # comment 2 image(x.coord, y.coord, matrix(pred &gt; 0.5, nrow = 100), col = grey.colors(2), main = &quot;size = 50 - decay = 0.0&quot;) points(x$train, col = x$class.train) # comment 3 mod_nnet &lt;- nnet::nnet(x$train, x$class.train == &quot;orange&quot;, size = 1, # size = H decay = 0.1, #decay = lambda maxit = 500, trace = FALSE, entropy = TRUE) # comment 4 pred &lt;- predict(mod_nnet, matrice.test) image(x.coord, y.coord, matrix(pred &gt; 0.5, nrow = 100), col = grey.colors(2), main = &quot;size = 1 - decay = 0.1&quot;) points(x$train, col = x$class.train) 13.4.4 Question 3 : Choix des hyperparamètres \\(H\\) et \\(\\lambda\\). Commenter le code suivant permettant de tester les prédictions de plusieurs modèles. Nous privilégierons un choix de \\(\\lambda\\) entre \\(0.8\\) et \\(10^{-3}\\) pour un nombre de neurones important (\\(H = 40\\)). # comment 1 logloss &lt;- function(p, y){ return(-y*log(p) - (1-y)*log(1-p)) } lambda &lt;- 10^{-seq(0.1,3,length = 30)} # comment 2 logloss_test &lt;- NULL for (l in lambda){ # comment 3 mod_nnet &lt;- nnet::nnet(x$train, x$class.train == &quot;orange&quot;, size = 40, decay = l, trace = FALSE, entropy = TRUE) # comment 4 logloss_test &lt;- c(logloss_test, mean(logloss(predict(mod_nnet, x$test), (x$class.test == &quot;orange&quot;))) ) } # comment 5 plot(lambda, logloss_test, log = &quot;x&quot;) # comment 6 lambda.cv &lt;- lambda[which.min(logloss_test)] # comment 7 mod_nnet &lt;- nnet::nnet(x$train, x$class.train == &quot;orange&quot;, size = 40, #H decay = lambda.cv, #lambda optimisé maxit = 500, trace = FALSE, entropy = TRUE) # prediction sur toutes la zone pred &lt;- predict(mod_nnet, matrice.test) image(x.coord, y.coord, matrix(pred &gt; 0.5, nrow = 100), col = grey.colors(2), main = &quot;size = 40 - decay = 0.06&quot;) points(x$train, col = x$class.train) Lorsque l’on utilise le paramètre de régularisation optimal, un affichage interactif permettant de zoomer l’image nous montre les détails des prédictions. image(x.coord, y.coord, matrix(pred, nrow = 100), col = terrain.colors(10), main = &quot;Carte de probabilité&quot;) points(x$train, col = x$class.train) ## Warning: package &#39;plotly&#39; was built under R version 3.4.1 ## Loading required package: ggplot2 ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout "]
]
