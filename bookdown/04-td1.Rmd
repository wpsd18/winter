# Travaux dirigés - séance 1


## Objectif de la séance



L'objectif de cette séance de travaux dirigés est de relier les notions de variance et de covariance aux concepts d'information et d'information mutuelle vus en théorie de l'information. 

Une idée centrale de nombreuses méthodes appliquées en science des données est que l'information est contenue dans la matrice de covariance des variables étudiées. Nous allons justifier cette idée sous l'hypothèse gaussienne, tout en démontrant que la loi de Gauss est la loi maximisant l'information disponible.


## Exercice 1. Théorie de l'information et loi normale 

L'objectif de cet exercice est d'établir le résultat suivant, justifiant l'importance de la loi normale dans la nature. 

**Principe d'entropie maximale** : La loi normale de densité $p(x) = N(x|0, \sigma^2)$ est la loi d'entropie maximale parmi toutes les lois de densité $q(x)$ dont l'espérance est nulle et dont la variance est égale à $\sigma^2$.


### Question 1

Considèrons une densité de probabilité, $p(x)$, définie sur l'ensemble des réels. On rappelle que l'information (ou [*entropie de Shannon*](https://en.wikipedia.org/wiki/Entropy_(information_theory))) de la densité de probabilité $p(x)$ est définie de la manière suivante

$$
h(p) = - \int_{\mathbb{R}} p(x) \log p(x) dx . 
$$
Dans cette équation, la fonction $\log$ est parfois définie en base 2, mais d'autres définitions sont possibles, et nous utiliserons par défaut la base naturelle ($e$). On suppose que $p(x)$ est la densité de la loi normale de moyenne $m$ et de variance $\sigma^2$, 

$$
p(x) = N(x | m , \sigma^2) \, , \quad x \in \mathbb{R}. 
$$

Montrer que l'entropie de Shannon de la loi normale est donnée par la formule suivante
$$
h(p) =  \frac12 \log ( 2 \pi e \sigma^2) . 
$$
_Indication_ : On utilisera à bon escient le théorème de transfert pour la variable aléatoire $\log p(x)$ où $x$ est de loi $N(m, \sigma^2)$.

### Question 2

Quelle interprétation de ce résultat vous parait être importante pour l'analyse d'une variable gaussienne de moyenne $m$ et de variance $\sigma^2$ ?  


### Question 3

On suppose désormais que les lois considérées sont centrées, c'est à dire d'espérance nulle. Nous supposons que les lois sont de variance finie égale à $\sigma^2$. Afin de démontrer le principe d'entropie maximale, nous considérons la [*divergence de Kullback-Leibler*](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) définie par  

$$
D_{KL}( q \|  p ) = \int_{\mathbb{R}} q(y) \log \left( \frac{q(y)}{p(y)} \right) dy \, .
$$

Rappeler l'interprétation de cette grandeur en terme de perte d'information (quitte à revenir à la définition rappelée dans le lien vers internet). À l'aide du théorème de transfert, démontrer que 

$$
D_{KL}( q \|  p ) = - h(q) - \mathbb{E}[\log p(y)] \, , 
$$
où $y$ est une variable aléatoire réelle de densité $q(y)$.

### Question 4

Soit
$$
p(x) = N(x | 0 , \sigma^2) \, , \quad x \in \mathbb{R}. 
$$ 
Par le calcul et en s'aidant de la question 1, en déduire que 
$$
D_{KL}( q \|  p ) = - h(q) + h(p) \, . 
$$
Pour cela, on se souviendra que la variance de la loi $q$ (qui est supposée centrée) est égale à $\sigma^2$ par hypothèse.

### Question 5

Conclure par l'application d'un résultat de la théorie de l'information que, pour toute loi $q$ d'espérance nulle et de variance $\sigma^2$, nous avons
$$
 h(q) \leq h(N(0, \sigma^2)) \, . 
$$



### Question 6 (facultative)

Généraliser le résultat précédent en dimension quelconque ($D > 1$) pour établir que la loi gaussienne $p = N(0, \Sigma)$ est la loi d'entropie maximale parmi toutes les lois $q$ dont l'espérance est nulle et dont la matrice de covariance est égale à $\Sigma$.

_Indication_ : On utilisera le théorème spectral pour établir que 
$$h(p) = \frac12 \log((2\pi e)^D |\Sigma|) $$
où $|\Sigma|$ est le déterminant de la matrice $\Sigma$.



## Exercice 2

L'objectif de cet exercice est de montrer que l'[*information mutuelle*](https://en.wikipedia.org/wiki/Mutual_information), $I$, de deux variables réelles $x$ et $y$ formant un couple gaussien est liée au coefficient de correlation de ces deux variables,

$$
\rho = \frac{{\rm cov}(x,y)}{\sqrt{\sigma^2_{\sf x} \sigma^2_{\sf y}}} \, , 
$$
par la relation suivante 

$$
I = -\frac12 \log(1 - \rho^2) \,.
$$



### Question 1 

Soit $x$ et $y$ deux variables aléatoires réelles de loi jointe $p(x,y)$ et de lois marginales $p_{\sf x}(x)$ et $p_{\sf y}(y)$. On appelle [information mutuelle](https://en.wikipedia.org/wiki/Mutual_information) la grandeur définie de la manière suivante

$$
I  =  \int p(x,y) 
\log \left( \frac{p(x,y)}{p_{\sf x}(x)p_{\sf y}(y)}  \right)  dx dy. 
$$

Montrer que 

$$
I =  \mathbb{E}[ \log p(x,y) ] + h(p_{\sf x}) + h(p_{\sf y}) 
$$
où $p_{\sf x}$ et $p_{\sf y}$ désignent respectivement les lois marginales des variables $x$ et $y$.


### Question 2

On suppose que $(x,y)$ est un couple de variables aléatoires de loi $N(0, \Sigma)$ où $\Sigma$ est une matrice de covariance quelconque

$$
\Sigma = \left( \begin{array}{cc} \sigma^2_{\sf x} & c_{\sf xy} \\
c_{\sf xy} & \sigma^2_{\sf y} \\
\end{array} \right) .
$$

Sous quelle condition (inégalité) la matrice $\Sigma$ définit-elle une matrice de covariance ? Nommer cette inégalité si possible.

### Question 3

Identifier les lois marginales $p_{\sf x}$ et $p_{\sf y}$ et calculer leur entropie. 

_Note_ : Admettre provisoirement que les lois maginales sont des lois normales. Cela sera vu dans le cours. Retenir ce résultat !  

### Question 4

En utilisant le théorème spectral, justifier que la matrice $\Sigma^{-1}$ peut s'écrire sous la forme 

$$
\Sigma^{-1} = {\bf U} {\bf \Lambda}^{-1} {\bf U}^T
$$
où ${\bf U}$ est une matrice carrée unitaire et ${\bf \Lambda}$ une matrice diagonale dont les termes sont strictement positifs (Rappel : ${\bf U}$ est une matrice unitaire si ${\bf U}{\bf U}^T={\bf I}$).

### Question 5 

On pose ${\bf x} = (x,y)^T$ et ${\bf z} = {\bf U}^T {\bf x}$. Démontrer que la matrice de covariance du vecteur ${\bf z} = {\bf U}^T {\bf x}$ est égale à ${\bf \Lambda}$. 

### Question 6

En déduire que
$$
\mathbb{E}[{\bf x}^T  \Sigma^{-1} {\bf x}] = 2 \, .
$$
_Note_ : Il est possible d'obtenir ce résultat directement, sans utiliser le théorème spectral, car on pourrait inverser une matrice $2\times 2$. L'interêt de la démonstration est de pouvoir être généralisée en dimension $D \geq 2$ (trouver le résultat). 


_Indication_ : Pour la démonstration, on pourra remarquer que 
$$ 
{\bf z}^T  \Lambda^{-1} {\bf z} = \sum_{j=1}^D \frac1{\lambda_j} z_j^2 \, ,
$$
ou mieux encore que
$$ 
{\bf z}^T  \Lambda^{-1} {\bf z} = {\rm Tr}(\Lambda^{-1} {\bf z} {\bf z}^T ).
$$




### Question 7

Démontrer que 
$$
I = -\frac12 \log(1 - \rho^2) \,.
$$

### Question 8

Interprétation.

