# Travaux dirigés 4

## Objectif de la séance 

L'objectif de cette séance de travaux dirigés est de construire un modèle de prédiction probabiliste lorsque les données de chaque catégorie sont échantillonnées selon des lois normales. Grâce à la formule de Bayes, nous établirons une correspondance entre ce modèle et un neurone formel probabiliste. 

On suppose que les observations (caractéristiques) constituent un vecteur ${\bf x}$ de dimension $D > 1$. Pour simplifier,  on suppose que les données se répartissent en deux classes, $y = 0$ et $y = 1$. Les données ont été générées en proportions $\pi_0$ et $\pi_1$ dans chacune des deux classes.

Le modèle génératif repose sur des lois gaussiennes de moyenne $\mu_k$, $k = 0, 1$, et de matrice de covariance $\Sigma$ *identique pour les deux classes*.

$$
p( {\bf x}  | y = k ) = N( {\bf x}  | \mu_k, \Sigma ) \, , \quad  \forall k = 0, 1, \quad {\bf x} \in \mathbb{R}^D.
$$



## Exercice. Analyse discriminante linéaire.
 
### Question 1 

En dimension 2, représenter graphiquement de manière idéalisée la distribution des deux classes en supposant $\Sigma = {\bf Id}$. Imaginer une règle simple pour décider quelle serait la classe d'une donnée prise dans un ensemble test.  

### Question 2

On suppose dans un premier temps que $\pi_0 = \pi_1$. Utiliser la formule de Bayes pour calculer la probabilité conditionnelle $p(y = 1 | {\bf x})$.

### Question 3

Rappeler la définition d'une fonction de prédiction neuronale (neurone formel probabiliste). Montrer que cette probabilité peut se représenter par la fonction *neuronale* suivante, appelée classifieur probabiliste (ou _soft_)

$$
p( y = 1 | {\bf x} ) = {\rm sigmoid}( {\bf w}^T  ( {\bf x} - {\bf x}_0) )  \, ,
$$

où
$$
{\bf w} = \Sigma^{-1} (\mu_1 - \mu_0) 
$$
et

$$
{\bf x}_0 = (\mu_0 + \mu_1)/2 \, .
$$

### Question 4

Montrer que la règle de décision optimale sépare les observations par un plan. Donner une interprétation géomètrique de cette règle.

### Question 5

On suppose maintenant que $\pi_0 \neq \pi_1$. Montrer que le calcul de ${\bf w}$ reste inchangé et que le calcul de ${\bf x}_0$ se modifie de la façon suivante

$$
{\bf x}_0 = \frac12 (\mu_0 + \mu_1) - (\mu_1 - \mu_0) \frac{\ln(\pi_1/\pi_0)}{(\mu_1 - \mu_0)^T \Sigma^{-1} (\mu_1 - \mu_0)} \, .
$$

### Question 6

On effectue la simulation décrite ci-dessous. Le résultat est contenu dans le vecteur `x`.

```{r td4_lda_1}
# Simulation d'un échantillon de données 
    n = 200
    C = matrix(c(1,1,1,2), nrow = 2)
    
# Tirage des classes equiprobables selon la loi binomiale
    n_blue <- rbinom(1, size = n, prob = 1/2) 
    class <- c(rep("blue", n_blue), rep("orange", n - n_blue))
  
# Tirages gaussiens à l'intérieur des classes  
    x_0 <- mvtnorm::rmvnorm(n_blue, mean = c(1,0), sigma = C)
    x_1 <- mvtnorm::rmvnorm(n - n_blue, mean = c(0,1), sigma = C)
    
    x <- rbind(x_0, x_1)
    plot(x, col = class, cex = 1.2, pch = 19)
```

* Décrire en termes mathématiques les lois génératives de la simulation.  

* Proposer un moyen simple pour estimer les moyennes de classe ($\mu_0$ et $\mu_1$), la matrice de covariance $\Sigma^{-1}$ à partir des données d'apprentissage. Commenter le code R ci-dessous (définissant aussi un ensemble test).

```{r td4_lda_2}
# comment 1 : 
  samp = sample(1:n, n/2) 
  x_train <- x[samp,] ; class_train <- class[samp]
  x_test <- x[-samp,] ; class_test <- class[-samp]
  
# comment 2 : 
   mu0 <- apply(x_train[class_train == "blue",], 2, mean)
   mu1 <- apply(x_train[class_train == "orange",], 2, mean)

# comment 3 : 
   Sigma0 <- cov(x_train[class_train == "blue",]) 
   Sigma1 <- cov(x_train[class_train == "orange",])
```

* Calculer les vecteurs ${\bf w}$ et ${\bf x}_0$ et calculer les prédictions optimales pour l'ensemble test à partir de l'ensemble d'apprentissage. 

```{r td4_lda_3}
sigmoid <- function(x) {1/(1 + exp(-x))}

# comment 4 :
  Sigma <- (Sigma0 + Sigma1)/2
  w <- solve(Sigma) %*% (mu1 - mu0)
  x0 <- (mu0 + mu1)/2

# comment 5 : (décommenter et remplacer)
  # pred <- sigmoid(t(w) %*% t(change_moi))
  # pred <- as.numeric(pred)
```


```{r include = FALSE}
   pred <- as.numeric(sigmoid(t(w) %*% t(x_test - x0)))
```

### Question 7

Comparer le résultat à la fonction `lda()` de R. Cette fonction cherche un plan séparant les deux classes selon un critère d'optimisation statistique. Commenter le code R suivant. 

```{r td4_lda_mass}
  mod.lda <-  MASS::lda(class_train ~ ., data = data.frame(x_train), prior = c(0.5, 0.5))

# comment 1
  pred.lda <- predict(mod.lda, data.frame(x_test))$posterior[,2]

# comment 2
  plot(as.numeric(pred), pred.lda)
  abline(0,1, col = 2, lwd = 3)
```


### Question 8

Calculer les valeurs des fonctions d'erreur logloss et 01 sur l'ensemble test à l'aide d'un code R.


### Réponse à la question 8


```{r}
# log-loss
  - mean((class_test == "blue")*log(1 - pred) + (class_test == "orange")*log(pred))
```


```{r}
# Matrice de confusion :
  table(pred > 0.5, class_test)
```



