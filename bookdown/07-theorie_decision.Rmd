# Théorie de la décision

```{r, include= FALSE}
library(magrittr)
```


## Introduction et objectifs 

Ce chapitre et le chapitre suivant forment le coeur de ce cours. Trois séances d'amphi seront consacrées à ces deux chapitres. 

En apprentissage automatique, un prédicteur probabiliste sera défini comme une fonction calculant des probabilités conditionnelles pour chaque observation à classer.  Un algorithme d'apprentissage ajustera le modèle de prédiction en  minimisant une fonction de perte calculée à partir des données observées. Les fonctions de perte les plus utilisées sont l'erreur de classification et la perte d'information. 

Nous verrons comment utiliser la formule de Bayes pour déterminer le prédicteur probabiliste optimal, en supposant un échantillon de taille infinie. Nous verrons aussi que le phénomène de surapprentissage apparaît lorsque la taille de l'échantillon d'apprentissage est petite devant le nombre de paramètres du modèle de prédiction et nous verrons comment traiter ce problème en calculant la fonction de perte sur un ensemble de données n'ayant pas servi lors de la phase apprentissage.  

* Formule de Bayes 
* Taux de bonne classification (_accuracy_) et perte d'information (_logloss_)
* Classifieur optimal 
* Problème du sur-apprentissage et recours à un ensemble de test 

## Quelques notions probabilistes supplémentaires

Nous définissons les notions d'espérance conditionnelle, la formule de Bayes, ainsi que les formules de conditionnement. Ces notions élémentaires seront très utiles pour la bonne compréhension des méthodes employées par la suite.    

### Espérance conditionnelle

On considère un couple formé d'un vecteur et d'une variable aléatoire, $({\bf x},y)$, associant un vecteur de mesures multidimensionnelles, ${\bf x}$, à une variable discrète, $y$. La variable $y$ représente typiquement une catégorie pour ce vecteur ($y \in \{1,\dots,K\}$). Par exemple, ${\bf x}$ peut être une image et $y$ un contenu pour cette image (un chat, un chien ou une autre chose). 

Nous souhaitons calculer l'espérance d'une variable aléatoire à valeurs réelles $z$ définie comme une fonction du couple précédent

$$
z = f({\bf x}, y) \, .
$$

**Définition.** L'*espérance conditionnelle* de $z$ sachant ${\bf x}$ se calcule en toute généralité de la manière suivante
$$
\mathbb{E}[z | {\bf x} ]  = \int z p(z | {\bf x}) dz = \int f({\bf x},y) p( y| {\bf x}) dy \, .
$$

Lorsque la variable $y$ est discrète et prend ses valeurs parmi $K$ modalités, l'intégrale peut être remplacée par une somme

$$
\mathbb{E}[z | {\bf x} ]  = \sum_{k = 1}^K f({\bf x},k) p( y = k| {\bf x}) \, .
$$

**Formule de conditionnement.** Pour obtenir l'espérance de $z$, il suffit de sommer sur toutes les valeurs possibles de ${\bf x}$

$$
\mathbb{E}[z]  =  \int  \mathbb{E}[z | {\bf x} ] p({\bf x}) d {\bf x} .
$$
En utilisant la formule de transfert (cours 1), nous pouvons écrire cette formule de manière probabiliste

$$
\mathbb{E}[z]  =   \mathbb{E}[  \mathbb{E}[z | {\bf x} ]].
$$


Pour comprendre la logique de cette formule, on pourrait retenir l'exemple suivant. Supposons qu'un cours  comporte deux groupes, ayant respectivement 12 et 28 inscrits. À l'examen, la moyenne du premier groupe est égale à 12 et la moyenne du second groupe est égale à 14. On se demande alors quelle est la moyenne globale des inscrits. Une mauvaise réponse (égale à 13) consiste à faire la moyenne des deux notes. La bonne réponse prend en compte la probabilté de se trouver dans chaque groupe et elle est égale à $12 \times 12/40 + 13 \times 28/40 = 13.4$. La formule est analogue à la formule des probabilités totales, appliquée au calcul l'espérance directement.

### Formule de Bayes

Sous les mêmes conditions que dans la section précédente, supposons que $y$ est une variable discrète prenant $K$ valeurs. La *formule de Bayes* permet de calculer la probabilité conditionnelle $p(y = \ell|{\bf x})$  à partir de la loi conditionnelle du vecteur ${\bf x}$ sachant la valeur  $y = \ell$. La formule s'écrit de la manière suivante 

$$
p(y = \ell|{\bf x}) = \frac{p({\bf x} | y = \ell ) p(y = \ell)}{p({\bf x})} = \frac{p({\bf x} | y = \ell ) p(y = \ell)}{\sum_{k = 1}^K p({\bf x} | y = k ) p(y = k) } \, , \quad \ell = 1, \dots, K.
$$


## Fonctions d'évaluation de classification

### Classification dure ou probabiliste (douce)

Les fonctions d'évaluation de classification, aussi appelées _fonctions de perte_ (loss function), permettent de mesurer la pertinence des classements effectués par des méthodes automatiques, appelées *machines*. Les fonctions d'évaluation, _ou fonctions de perte_, sont essentielles pour définir les algorithmes d'apprentissage automatique et pour évaluer les performances des "machines" apprennantes. 

Dans ce cours, les algorithmes d'apprentissage supervisés sont essentiellement des algorithmes de minimisation d'une fonction de perte. Il existe de nombreuses fonctions de perte. Dans les algorithmes classiques, les fonctions les plus souvent utilisées sont les fonctions _accuracy_ et _logloss_ dont nous verrons les définitions un peu plus loin. 


En apprentissage supervisé, on distingue deux types d'évaluation selon que l'on s'intéresse à une classification *dure* (_hard classifier_) ou *probabiliste* (_soft classifier_).  On parle de *classification dure* lorsque l'on construit une fonction (ou "machine") classant un vecteur de mesures observées ${\bf x}$ dans une catégorie choisie parmi $K$ possibles

$$
 { \bf x }  \mapsto c({ \bf x })  \in \{ 1, \dots, K\} \, .
$$
Une fonction de perte est une fonction $L(c({\bf x}),y)$ donnant une valeur à l'erreur commise lorsque la machine remplace $y$ par $c({\bf x})$. 

On parle de *classification probabiliste* (ou douce) lorsque l'on construit une fonction calculant les probabilités de chacune des $K$ classes possibles


$$
 { \bf x }  \mapsto q({ \bf x }) = (q_1({ \bf x }), \dots, q_K({ \bf x }))   \in [0,1]^K \, , \quad {\rm t.q.} \quad \sum_{k = 1} q_k({ \bf x }) = 1 \, .
$$


Lorsque le classifieur est probabiliste, il est courant de convertir la valeur $y$ comprise entre 1 et $K$ en un vecteur unitaire de longueur $K$, dont les coordonnées sont toutes nulles sauf la $y$-ème coordonnée. Cette dernière est égale à 1. Une fonction de perte est alors une fonction $L(q({\bf x}), y)$ mesurant l'erreur ou la perte d'information induite par l'approximation du vecteur unitaire par $q({\bf x})$.  


### Erreur de classification (01-loss)

La plus simple des fonctions de perte consiste à compter les erreurs de classification commises par une machine dure. Nous verrons que la machine dure minimisant cette erreur s'appuie directement sur une machine probabiliste. Les machines considérées par la suite seront donc essentiellement des machines probabilistes.

L'erreur de classification d'une machine dure, $c({\bf x})$, est définie par

$$
 L(c({\bf x}), y) = \left\{  \begin{array}{l} 1  & {\rm si~} c({\bf x}) \neq y, \\ 
0  & {\rm sinon.}
\end{array} \right. 
$$

L'*erreur moyenne de classification* d'une machine dure définie par $c({\bf x})$ est égale à


$$
E = \mathbb{E} [ L(c({\bf x}),y) ]  \approx \frac1n \sum_{i = 1}^n \mathbb{1}_{c({\bf x}_i) \neq y_i} \, .
$$

Cette erreur est très simple à calculer. Dans l'échantillon considéré $({\bf x}_i, y_i)_{i=1,\dots,n}$, on compte le nombre de fois que la catégorie apprise ne coïncide pas avec la valeur observée. On divise ensuite le résultat par la taille de l'échantillon $n$. Dans les logiciels d'apprentissage, le nombre moyen de bonnes classifications est appelé _accuracy_. La valeur de la mesure _accuracy_ est égale à $1 - E$.

### Matrice de confusion

Il est courant de s'intéresser plus précisement aux erreurs de classification commises par un classifieur dur. Pour cela, la notion de *matrice de confusion* est utile. Pour simplifier, considérons le cas d'un problème de classification en deux classes, notées $0$ ou $1$ (pour changer un peu). La matrice de confusion est la matrice de taille $2\times 2$ définie de la manière suivante 

|   | y = 0  | y = 1  |
|:---: |:---|:---|
|$c({\bf x}) = 0$  | TN | FN |
|$c({\bf x}) = 1$  | FP | TP |


Considérant la valeur 1 comme une classification positive et la valeur 0 comme une classification négative, FP et FN dénombrent les faux-positifs et les faux-négatifs. TP et TN dénombrent les vrais-positifs et les vrais-négatifs. L'erreur de classification correspond à l'addition des termes hors diagonale, FP + FN.

Des fonctions de perte plus générales que l'erreur de classification simple peuvent tenir compte de la nature des erreurs de classification en pondérant les contributions respectives des grandeurs FP et FN dans le calcul de la perte.


## Règle de classification dure optimale

Le résultat concernant la machine dure optimale s'énonce de la manière suivante.

**Résultat.** Considérons la fonction de perte définie par l'erreur de classification (perte 01). Pour cette fonction de perte, la machine dure optimale est donnée par le calcul des probabilités conditionnelles des classes sachant les variables observées

$$
c_{\rm opt}({\bf x})  = \arg\max_k { p(y = k | {\bf x}) } \,.
$$

En clair, la machine optimale choisit parmi les classes $\{1,\dots, K\}$ la classe ayant la plus grande probabilité conditionnelle, $q_k({\bf x}) = p(y = k | {\bf x})$. Nous voyons ainsi que la machine dure optimale s'appuie sur la machine probabiliste $q({\bf x}) = p(.|{\bf x})$. 

**Preuve.** La preuve de ce résultat est simple et repose sur la loi de Bernoulli. Considérons un classifieur dur quelconque, $c({\bf x})$. D'après la formule de conditionnement, nous avons

$$
\mathbb{E} [ L(c({\bf x}),y) ] =  \int \mathbb{E} [ L(c({\bf x}),y) | {\bf x}] p({\bf x}) d{\bf x}  \, .
$$
Pour minimiser cette expression, il suffit de le faire pour tout vecteur ${\bf x}$ à l'intérieur de l'intégrale. Cela revient à minimiser la grandeur

$$
\mathbb{E} [ L(c({\bf x}),y) | {\bf x}] = p(y \neq c({ \bf x}) | {\bf x}) = 1 - p(y = c({ \bf x}) | {\bf x}).
$$
On peut donc choisir $c({\bf x})$ de sorte à maximiser la probabilité $p(y = c({ \bf x}) | {\bf x})$. Pour maximiser cette grandeur, il y a un nombre fini de cas à évaluer. En effet, supposons que $c({\bf x}) = k$, nous avons

$$
p(y = c({ \bf x}) | {\bf x}) =  p(y = k | {\bf x}) \, .
$$
Nous voyons ainsi que le choix optimal peut s'écrire de la manière suivante

$$
c_{\rm opt}({\bf x})  = \arg\max_{k \in [1,K]} { p(y = k | {\bf x}) } \,.
$$

La stratégie de classification optimale consiste à choisir la catégorie $k$ maximisant la probabilité conditionnelle. Cette règle est souvent appelée la _règle de classification de Bayes_, en référence à la formule de Bayes que l'on peut utiliser pour le calcul de la probabilité conditionnelle apparaissant dans la formule. 


### Exemple unidimensionel

Considérons un exemple unidimensionel dans lequel des variables positives appartiennent à deux catégories équiprobables, numérotées $y = 0$ et $y = 1$. Les variables sont créées selon un modèle génératif de loi exponentielle, dont le paramètre est connu et spécifique à la classe des variables ($\lambda_0$ ou $\lambda_1$ selon que $x$ appartient à la catégorie $0$ et $1$). Pour tout $x>0$, les densités des lois conditionnelles sont données par les formules suivantes
$$
p(x|y = 0) = \lambda_0 \exp(-\lambda_0 x )  
$$
et 
$$
p(x|y = 1) = \lambda_1 \exp(-\lambda_1 x ) \, .
$$
Dans ce cas, nous pouvons appliquer la formule de Bayes pour définir le classifieur optimal. Puisque la variable de classe prend la valeur $0$ ou $1$, il suffit de calculer la probabilité conditionnelle $p(y = 1 |x)$. La probabilité $p(y = 0 |x)$ s'obtient par complémentaire, $p(y = 0 |x) = 1 - p(y = 1 |x)$.

D'après la formule de Bayes, nous avons

$$
p(y = 1 |x) = \frac{p(x|y=1) p(y = 1)}{p(x|y=0) p(y = 0) + p(x|y=1) p(y = 1)}\,.
$$

Puisque les classes sont équiprobables, nous pouvons supposer que $p(y=1) = p(y=0)$. En remplaçant les densités par leur expression respective, nous obtenons
$$
p(y = 1 |x) = \frac{1}{1 + \exp(-(\lambda_0 - \lambda_1)x - \log(\lambda_1/\lambda_0))}\,.
$$

la règle de classification optimale consiste à classer $x$ en dans la catégorie 1 si 
$$
p(y = 1 |x) > p(y = 0 |x) \, ,
$$
c'est à dire, si $p(y = 1 |x) > 1/2$. Cette condition est équivalente à choisir la catégorie 1 lorsque la variable $x$ est supérieure à un seuil $s$, décrit ci-dessous

$$
x > s = \frac{\log(\lambda_0) - \log(\lambda_1)}{\lambda_0 - \lambda_1} .
$$
La valeur $x = s$ est appelé la _frontière_ de décision entre les classes 0 et 1.

### Simulation

L'exemple précédent peut être simulé très facilement et nous renseigner sur une borne atteignable pour l'erreur de classification moyenne. Pour fixer les idées, prenons les valeurs $\lambda_0 = 1/10$ et $\lambda_1 = 1/2$.

Dans ce cas, la frontière de décision est donnée par l'équation $x = 4.02$, équivalente à $\log(x) = 1.39$. On attend donc que l'histogramme des valeurs de la variable $\log(x)$ soit séparé par le seuil $\log(x) = 1.39$ marquant la frontière entre les deux classes.

La simulation de cet histogramme peut s'écrire de la manière suivante

```{r cm4_hist_log_x}
  # nombre de simulations, taille de l'échantillon
  n <- 10000

  # simulation des variables x et y
  x <- c(rexp(n, rate = .1), rexp(n, rate = .5))
  y <- rep(0:1, each = n)
  
  # on passe en échelle log pour mieux voir les variations d'echelles
  hist(log(x), prob = TRUE)
  
  s <- (log(0.1) - log(0.5))/(0.1 - 0.5)
  points(log(s), 10, type = 'h', lwd = 3, col = 2)
```

Nous voyons ainsi que la frontière de décision optimale n'est pas intuitive. Dans cet exemple, la valeur de l'erreur de classification moyenne réalisée par le classifieur optimal n'est **pas égale à zéro**. Elle peut être estimée assez précisement en augmentant le nombre de simulation ($n \to \infty$). Nous obtenons une valeur approchée, située autour de 23\% de la manière suivante  

```{r}
# Erreur de classification minimale 
  mean(y != (x < s))
```



## Classification probabiliste optimale

Nous venons de voir que la machine optimale au sens de l'erreur de classification moyenne s'appuie sur un classifieur probabiliste de la forme $q_k({\bf x}) = p(y = k | {\bf x})$.

$$
c_{\rm opt}({\bf x})  = \arg\max_k { p(y = k | {\bf x}) } \,.
$$

La plupart des algorithmes d'apprentissage définissent directement un classifieur probabiliste.  Pour cela, d'autres fonctions de perte que l'erreur de classification peuvent être proposées. Les fonctions de perte les plus courantes sont l'entropie-croisée, aussi appelée _logloss_ ou tout simplement _entropie_, et l'erreur quadratique moyenne, aussi appelée erreur $L^2$.   


### Perte log-loss (entropie croisée)

La fonction de perte _log-loss_  est très fréquemment considérée par les algorithmes d'apprentissage probabiliste. Elle est définie de la manière suivante  

$$
L(q({\bf x}), y) = - \log q_y({\bf x}) \, , \quad y \in 1, \dots, K.
$$
Lorsque la variable $y$ est binaire, la fonction s'écrit aussi de la manière suivante

$$
L(q({\bf x}), y) = - y \log q({\bf x}) - (1-y)\log (1 - q({\bf x})) \, ,
$$
où $q({\bf x})$ est identifié à la probabilité de la classe 1, $q({\bf x}) = q_1({\bf x})$.


La perte moyenne associée à la fonction de perte **log-loss** est égale à

$$
\mathbb{E}[L(q({\bf x}), y )] =  \int \mathbb{E}[L(q({\bf x}), y) | {\bf x}] p({\bf x}) d{\bf x} \, .
$$

Pour ce critère, nous obtenons que la machine probabiliste optimale calcule la probabilité conditionnelle de chaque catégorie.

**Résultat :** L'espérance de la perte log-loss est minimale si et seulement, 

$$
q_k({\bf x}) = p(y = k|{\bf x}) \, , \quad k = 1, \dots, K.
$$
Nous allons démontrer ce résultat très important pour le cours. Nous verrons aussi que minimiser la perte moyenne log-loss revient à minimiser l'information perdue lorsque l'on approche la loi cible $p(y|{\bf x})$ par une machine probabiliste $q({\bf x})$. 

Pour minimiser la fonction de perte moyenne définie ci-dessus, il suffit de minimiser le terme intervenant à l'intérieur de l'intégrale. Cela revient à rechercher une machine probabiliste $q({\bf x})$ minimisant l'espérance conditionnelle suivante

$$
\mathbb{E}[L(q({\bf x}), y) | {\bf x} ] = - \sum_{k = 1}^K p(y = k|{\bf x}) \log q_k({\bf x}) \, .
$$

 
Rappelons que pour deux lois de probabilité $p_1$ et $p_2$ sur l'ensemble $\{1,\dots,K\}$, l'entropie croisée est définie de la manière suivante

$$
h( p_1 , p_2 ) = - \sum_{k = 1}^K p_1(k) \log p_2(k) \, .
$$

Ainsi, la grandeur qui apparaît dans le terme de droite de l'expression précédente correspond à l'_entropie croisée_ des lois $p(y = k|{\bf x})$ et $q_k({\bf x})$.

$$
h( p_1 , p_2 ) = - \sum_{k = 1}^K p_1(k) \log p_2(k) \, .
$$

Nous pouvons comparer cette grandeur à l'entropie de la loi conditionnelle $p(y = k|{\bf x})$

$$
h(p_{\sf y|{\bf x}} ) = - \sum_{k = 1}^K p(y = k|{\bf x}) \log p(y = k|{\bf x}).
$$

La différence des deux grandeurs est égale à la divergence de Kullback-Leibler 

$$
 h(p_{\sf y|{\bf x}} , q({\bf x}) ) - h(p_{\sf y|{\bf x}}) = D_{KL}(  p_{\sf y|{\bf x}} \| q({\bf x}) ) \, . 
$$

Puisque la divergence de Kullback-Leibler est toujours positive, nous obtenons que 

$$
h(p_{\sf y|{\bf x}}) = h(p_{\sf y|{\bf x}} , p_{\sf y|{\bf x}})  \leq  h(p_{\sf y|{\bf x}} , q({\bf x})) .
$$

En conclusion, la perte moyenne log-loss est minimale si et seulement, 

$$
q_k({\bf x}) = p(y = k|{\bf x}) \, , \quad k = 1, \dots, K.
$$

De plus, minimiser l'espérance conditionnelle $\mathbb{E}[L(q({\bf x}), y) | {\bf x} ]$ revient à minimiser la divergence $D_{KL}(  p_{\sf y|{\bf x}} \| q({\bf x}) )$. Ainsi, nous pouvons affirmer que minimiser la perte moyenne log-loss revient à minimiser l'information perdue lorsque l'on approche la loi cible $p(y|{\bf x})$ par un modèle probabiliste $q({\bf x})$. 


En pratique, la perte log-loss moyenne est calculée à partir des données d'apprentissage de la manière suivante 

$$
{\rm log~loss} \approx - \frac1n \sum_{i=1}^n \log( q_{y_i}({ \bf x}_i) )  \, .
$$

Lorsque la classification est binaire, cela revient à calculer

$$
{\rm log~loss} \approx - \frac1n \sum_{i=1}^n  y_i \log q({\bf x}_i)   + (1 -y_i) \log(1 - q({ \bf x}_i) ) \, . 
$$
Le cas binaire sera le cas le plus fréquemment rencontré dans les exemples de ce cours. La perte log loss est minimisée numériquement par les algorithmes d'apprentissage neuronaux que nous étudions en TP. 

### Erreur quadratique

Une seconde fonction d'erreur fréquemment considérée par les algorithmes d'apprentissage probabiliste est la fonction de perte quadratique ou perte $L^2$. Pour simplifier les notations, nous nous plaçons dans le cas d'une classification binaire. Lorsque la variable $y$ est binaire (0 ou 1), la fonction de perte s'écrit 

$$L(q({\bf x}), y) = ( y - q({\bf x}) )^2 ,$$
où $q({\bf x})$ s'identifie à la valeur $q_1({\bf x})$ pour la classe 1. Pour minimiser la perte quadratique moyenne, il suffit de trouver la machine probabiliste $q({\bf x})$ minimisant l'espérance conditionnelle suivante

$$
\mathbb{E}[L(q({\bf x}),y) | {\bf x} ] = \mathbb{E}[(y - q({\bf x}))^2 | {\bf x} ]  \, .
$$

Pour cela, faisons apparaître la probabilité conditionnelle de la classe 1, notée $p(y = 1 | {\bf x})$. Nous avons alors

$$
\mathbb{E}[(y - q({\bf x}))^2 | {\bf x} ]  =  \mathbb{E}[(y - p(y = 1 | {\bf x})) - (q({\bf x}) - p(y = 1 | {\bf x})) )^2 | {\bf x} ]   \, .
$$

En développant le carré, il est facile de voir que le terme correspondant au double produit est nul. Nous avons donc

$$
\mathbb{E}[(y - q({\bf x}))^2 | {\bf x} ]  =  \mathbb{E}[(y - p(y = 1 | {\bf x}))^2 | {\bf x} ]  + C({\bf x})^2  \, ,
$$
et, puisque le terme de droite est toujours positif, nous avons

$$
\mathbb{E}[(y - q({\bf x}))^2 | {\bf x} ]  \geq  \mathbb{E}[(y - p(y = 1 | {\bf x}))^2 | {\bf x} ]  \, .
$$


L'inégalité ci-dessus est valide pour tout modèle probabiliste $q({\bf x})$. Pour l'erreur quadratique moyenne, la machine probabiliste optimale correspond donc à nouveau à calculer la probabilité conditionnelle de chaque classe,

$$
q({\bf x}) =  p(y = 1 | {\bf x}) \, .
$$


Minimiser l'erreur quadratique moyenne conduit au même résultat que pour la perte log-loss. Ce résultat justifie l'utilisation de l'erreur quadratique moyenne par les algorithmes d'apprentissage. En pratique, cette erreur est calculée à partir des données empiriques de la manière suivante


$$
{\rm EQM} \approx  \frac1n \sum_{i=1}^n (y_i - q({\bf x}_i))^2 . 
$$

Pour généraliser ce résultat à $K$ classes, nous pouvons considérer un codage binaire de chaque classe. Ce type de codage est aussi appelé *factoriel* ou *catégoriel* (en statistique), *dummy variable* ou *one-hot encoding* en apprentissage automatique. La classe $k$ est codée par le vecteur  $(0,\dots, 1, \dots, 0)$  contenant des 0 et un (seul) 1 à la position $k$. Ce vecteur est de dimension $K$.

A l'aide du codage catégoriel, nous pouvons définir la fonction de perte correspondant à l'erreur quadratique de la manière suivante 


$$L(q({\bf x}), y) = L(q({\bf x}), (y_1,\dots, y_K)) =  \sum_{k=1}^K ( y_k - q_k({\bf x}) )^2 ,$$
où $y_k \in \{0,1\}$ pour tout $k = 1, \dots, K$.  

En suivant des arguments similaires à ceux utilisés pour deux classes, nous pouvons montrer que  la machine probabiliste optimale pour la fonction de perte moyenne correspond toujours à calculer la probabilité conditionnelle de chaque classe,

$$
q_k({\bf x}) =  p(y_k = 1 | {\bf x}) \, , \quad \forall k = 1 , \dots, K.
$$

En pratique, cette erreur est calculée à partir des données empiriques de la manière suivante


$$
{\rm EQM} \approx  \frac1n \sum_{i=1}^n \sum_{k=1}^K ((y_i)_k - q_k({\bf x}_i))^2 . 
$$

## Erreur d'apprentissage et erreur de prédiction

L'apprentissage automatique concerne essentiellement des machines ou des prédicteurs probabilistes. La phase d'apprentissage consiste en la minimisation d'un critère de perte à partir d'observations répétées du couple (variables, classe)  $({\bf x}_i, y_i)_{i = 1, \dots, n}$, supposées indépendantes les unes des autres.   

Un algorithme d'apprentissage peut prendre des formes très diverses. De nombreux algorithmes s'appuient sur la miminisation numérique des erreurs empiriques _log loss_ ou  _quadratique_, mais ce n'est pas le cas de toutes les méthodes. En général, un algorithme correspond à la construction d'une machine probabiliste, représentée par une fonction, $\hat q({ \bf x})$, telle que

$$
\hat q({ \bf x}) = \arg\min_q \frac1n \sum_{i = 1}^n L(q({\bf x}_i) , y_i) \,.
$$

Nous appelerons une telle machine probabiliste un _modèle de prédiction_.


### Modèles paramétriques 

Il y a deux grandes écoles de pensée pour la construction des _modèles de prédiction_ : les approches **paramétriques** et les approches **non-paramétriques**. Les premières regroupent des approches telles que les modèles gaussiens, la régression logistique, les réseaux de neurones, etc. Les secondes regroupent des approches telles que les méthodes de plus proches voisins, de distances, les méthodes à noyaux, les machines à vecteurs de support, etc. Ce cours met plutôt l'accent sur les méthodes paramétriques, tout en mentionnnant quelques approches non-paramétriques telles que les plus proches voisins.

Les _méthodes paramétriques_ utilisent un ensemble de valeurs numériques, $\theta$, pour paramétrer le modèle de prédiction. Elles supposent que le modèle est une fonction de ${\bf x}$ et $\theta$ 

$$
q({\bf x}) = q({\bf x}, \theta) \,. 
$$

Rechercher la machine optimale par un algorithme d'apprentissage consiste alors à rechercher le paramètre $\theta$ minimisant la fonction $L(\theta)$ suivante

$$
L(\theta) = \frac1n \sum_{i = 1}^n L(q({\bf x}_i, \theta), y_i) \,.
$$
Le minimum obtenu sera noté $\hat \theta$ 

$$
\hat \theta = \arg\min_\theta L(\theta) \,.
$$

Le _modèle de prédiction ajusté_ sera alors obtenu en injectant la valeur calculée, $\hat \theta$, dans le modèle de prédiction

$$
\hat q({\bf x}) = q({\bf x}, \hat \theta) \, .
$$ 

Lorsque la fonction de perte et le modèle de prédiction sont (plusieurs fois) différentiables, il est possible d'utiliser des algorithmes d'optimisation numériques tels que l'_algorithme du gradient_ ou des _méthodes de Newton_ pour trouver les minima de la fonction de perte empirique. 

Dans ce cours, nous utiliserons des algorithmes d'optimisation implantés dans des bibliothèques numériques. L'étude des méthodes d'optimisation n'est pas le sujet du cours, mais il faut retenir qu'il est très important de bien les choisir et qu'il s'agit d'un point important à approfondir si l'on s'intéresse au domaine de l'intelligence artificielle.  


Pour illustrer la démarche d'optimisation, considérons à nouveau les données unidimensionnelles générées par des lois exponentielles pour une classification binaire, introduites dans une section précédente. Pour cet exemple, définissons le modèle de prédiction suivant 

$$
q(x, \theta_1, \theta_2) = \frac{1}{ 1 + \exp(-\theta_1x-\theta_2) } \, .
$$

Pour ce problème, nous pouvons créer une fonction R calculant $L(\theta)$ en nous appuyant sur la perte logarithmique _log-loss_. Cela peut se faire  de la manière suivante 

```{r}
L <- function(theta){
  # les variables x et y sont des variables globales
  # calcul du critère de décision \theta_1 * x + \theta_2
  z <- theta[1]*x + theta[2]
  # calcul de la probabilité de la classe 1 
  prob <- 1/(1 + exp(-z))
  # renvoie la perte log-loss moyenne
  return(-mean(y*log(prob) + (1-y)*log(1-prob)))
}
```

Il est possible de minimiser cette fonction en utilisant la commande _optim()_ de R. Cette fonction constitue un exemple de programmation fonctionnelle. Pour cela nous utilisons un algorithme d'optimisation numérique assez classique, appelé ["méthode BFGS"](https://fr.wikipedia.org/wiki/BFGS), que vous étudierez peut être dans un autre cours,

```{r}
  # minimisation de la fonction L(theta)

  # condition initiale theta1 = -1, theta2 = 2
    objet_optim <- optim(par = c(-1,2), fn = L, method = "BFGS")

  # solution dans l'attribut "par"
    objet_optim$par %>% round(2)
```

 Le calcul théorique de la probabilité $p(y = 1 |x)$ a été effectué dans la section précédente. Il nous indique que la solution optimale est $\theta_1 = - 0.4$ et $\theta_2 = \log(5) = 1.60$. Les valeurs numériques obtenue ci-dessus sont donc proches des valeurs obtenues théoriquement en considérant un échantillon de taille $n = \infty$.

### Méthodes non-paramétriques (à compléter)

Les méthodes non-paramétriques tentent d'estimer les probabilités conditionnelles des classes par des approches locales. Elles s'appuient sur un nombre très restreint de paramètres. La méthode des _k plus proches voisins_ est une illustration directe de telles approches. 

![Méthode des k plus proches voisins : Les variables observées pour deux catégories "bleues" et "rouges" correspondent aux carrés et aux triangles. On détermine la probabilité de la classe au point vert par un sondage des points voisins du point vert. La probabilité d'appartenir à la catégorie "rouge" est 2/3 pour le premier cercle, 2/5 pour le second cercle. Le nombre de voisins considérés est un paramètre du modèle. La méthode s'avère inefficace en grande dimension à cause du fléau de la dimension.](./figures/Knn.png)

Dans la méthode des plus proches voisins, on décide du calcul d'une distance entre les variables. On détermine alors la probabilité pour qu'un vecteur ${\bf x}$ appartienne à une catégorie donnée en sondant les $k$ vecteurs de l'échantillon les proches de ${\bf x}$. La méthode de sondage peut parfois introduire des pondérations des votes en fonction de la distance au vecteur cible ${\bf x}$. Cette pondération est appelée _noyau_ et plusieurs formes de noyau peuvent être proposées dans les méthodes d'apprentissage automatique.

### Malédiction de la dimension (à développer)

Donner l'exemple du cube unité. Calculer la longueur du coté, $a$, nécessaire pour occuper 10$\%$ du volume total dans le volume unité. Voir que $a \to 1$ rapidement lorsque la dimension du cube grandit.

### Ensemble test

Nous revenons aux modèles paramétriques qui constituent la cible de ce cours, et nous abordons le problème de l'_ensemble test_. Jusqu'à maintenant nous avons considéré que les données observées pouvaient être utilisées pour l'apprentissage des machines. Dans cette section, nous allons voir qu'une partie des données disponibles doit être réservée pour évaluer la fonction de perte indépendamment de l'ensemble ayant servi pour ajuster le modèle (apprentissage).


Les modèles paramétriques peuvent nécessiter d'ajuster un très grand nombre de paramètres. Nous verrons dans la suite que ce nombre peut être gigantesque pour les réseaux de neurones multicouches. Dans ce cas, la taille de l'échantillon peut être insuffisante pour correctement évaluer la valeur moyenne de la fonction de perte et le phénomène de sur-apprentissage ou sur-paramétrage (overfitting) apparait. 

![Illustration du phénomène de sur-apprentissage pour un problème de classification (rouge/bleu). La fontière noire correspond à la frontière de décision optimale. La frontière verte correspond à la frontière calculée par un modèle sur-paramétré. Source de l'image: wikipedia](./figures/Overfitting.png)



Pour comprendre le sur-apprentissage, revenons en dimension $D=1$ et supposons que l'on dispose de $n$ points répartis en $2$ catégories binaires (0 ou 1). Supposons que l'on souhaite ajuster le modèle de prédiction $q(x, \theta)$ décrit ci-dessous, possédant $2n$ paramètres 

$$
q(x, \theta) = \sum_{i = 1}^n \theta_{1,i} \delta_{\theta_{2,i} } (x) \,  .
$$

Pour ajuster ce modèle, nul besoin d'algorithme numérique. La solution évidente consiste à poser $\hat\theta_{1,i} = y_i$ et $\hat\theta_{2,i} = x_i$, pour tout $i$ de 1 à $n$. Dans ce cas, nous avons

$$
q(x_j, \theta) = \sum_{i = 1}^n \theta_{1,i} \delta_{\theta_{2,i} } (x_j)  = y_j \, , \quad j = 1, \dots, n.
$$

La classification est donc parfaite. Les erreurs de classification et de moindres carrés et la perte log loss sont égales à zéro, la valeur minimale absolue, pour l'ensemble d'apprentissage constitués des $n$ observations.

Cette machine (inutile) montre qu'il est toujours possible de construire un algorithme obtenant une erreur nulle sur l'ensemble d'apprentissage en considèrant suffisamment de paramètres dans un modèle. Certains modèles, dits flexibles ou peu contraints, pourront montrer des problèmes similaires, comme illustré dans l'image ci-dessus. Sauf si l'ensemble d'apprentissage est infiniment grand, nous comprenons qu'il faut être vigilant lors de l'évaluation un prédicteur sur l'ensemble d'apprentissage. 

La suite de ce paragraphe nous montre que la machine inutile n'a effectivement aucun intérêt prédictif. Nous avons vu précédement que la grandeur cible, que l'on cherche à minimiser, est l'erreur théorique suivante

$$
E = \mathbb{E}[ L( q(x) , y )] \,.
$$

Pour la machine que nous venons de construire, l'espérance $E$ est calculable théoriquement. Le résultat est égal à la probabilité $p(y = 1)$ pour la perte quadratique moyenne ou pour l'erreur de classification moyenne. Il est égal $+\infty$ pour l'erreur log-loss moyenne. Nous voyons donc que l'erreur moyenne n'est pas égale à zéro (sauf si $p(y = 0) = 1$). Si les classes sont équiprobables, les performances théoriques de l'algorithme d'apprentissage sont très mauvaises. Nous pouvons aussi vérifier ce résultat avec une erreur calculée empiriquement sur un échantillon test indépendant de l'échantillon d'apprentissage. 

**Règle importante :** En général, on réservera un sous echantillon pour minimiser l'erreur d'apprentissage et un second sous-echantillon, composé de données non-utilisées pendant l'apprentissage, pour évaluer l'erreur ou la perte $E$. Selon la taille du problème, on peut se permettre de réserver un pourcentage plus ou moins élevé de données pour l'ensemble de test (20$\%$ semble être un compromis moderne acceptable). Il sera alors important de faire varier la complexité du modèle et de choisir le modèle produisant la plus petite erreur moyenne sur l'ensemble test.  

![Erreurs d'apprentissage vs erreur de test en fonction de la complexité d'un modèle. L'erreur calculée sur l'ensemble test (courbe rouge) est en général plus élevée que l'erreur calculée sur l'ensemble d'apprentissage. Le meilleur choix de modèle correspond au minimum de l'erreur calculée sur l'ensemble test. Source de l'image: wikipedia.](./figures/Overfitting2_1.png)


```{r fig.width=2, fig.height=2, echo=FALSE, fig.cap="Erreurs d'apprentissage vs erreur de test en fonction de la complexité d'un modèle. L'erreur calculée sur l'ensemble test (courbe rouge) est en général plus élevée que l'erreur calculée sur l'ensemble d'apprentissage. Le meilleur choix de modèle correspond au minimum de l'erreur calculée sur l'ensemble test. Source de l'image: wikipedia."}
#library(png)
#library(grid)
#  img <- readPNG("bookdown/figures/Overfitting3.png")
#  grid.raster(img)
```


### Exemple de séparation linéaire

On considère un vecteur de mesures, ${\bf x}$, de dimension $D = 2$. On suppose que les données se répartissent en deux classes, $y = 0$ et $y = 1$. On suppose de plus que les données ont été générées en proportions égales dans chacune des deux classes, 
$$
p(y = 0) = p(y = 1).
$$

Nous étudions un modèle génératif reposant sur des lois conditionnelles gaussiennes de moyenne respectives $\mu_0 = (1,0)$ pour la classe 0 et $\mu_1 = (0,1)$ pour la classe 1. La matrice de covariance $\Sigma$ est supposée identique pour les deux classes. Dans notre exemple, cette matrice est égale à 

$$
\Sigma = \left( \begin{array}{cc} 1 & 1 \\
1 & 4 \\
\end{array} \right) .
$$

Le modèle de génération de données se décrit donc de la manière suivante

$$
p( {\bf x}  | y = k ) = N( {\bf x}  | \mu_k, \Sigma ) \, , \quad  \forall k = 0, 1, \quad {\bf x} \in \mathbb{R}^2.
$$

Supposons que les moyennes $\mu_k$ et la matrice $\Sigma$ sont connues ou estimées sans erreur. Pour ce modèle génératif, comme pour l'exemple exponentiel vu précédemment, nous pouvons déterminer exactement la probabilité conditionnelle de chaque classe. Le calcul conduit à la formule suivante, que nous retrouverons en travaux dirigés,

$$
p( y = 1 | {\bf x} ) = \frac{1}{1 + \exp( - {\bf w}^T  ( {\bf x} - {\bf x}_0) )}  \, ,
$$

où
$$
{\bf w} = \Sigma^{-1} (\mu_1 - \mu_0) \, , 
$$
et
$$
{\bf x}_0 = (\mu_0 + \mu_1)/2 .
$$

Avec les valeurs choisies pour exemple, la frontière de décision optimale est la droite d'équation $x_2 = -3/4 + 5 x_1/2$, correspondant à

$$
{\bf w}^T ({\bf x}  - {\bf x}_0) = 0.
$$
Nous pouvons visualiser la séparation des classes à l'aide d'une simulation comportant 200 données

```{r, include=FALSE}
library(magrittr)
```


```{r}
# definition de la matrice de covariance
  Sigma <- c(1,1,1,4) %>% matrix(nrow = 2)
# génération des données de chacune des deux classes (2xn)
  n <- 100
  x_0 <- MASS::mvrnorm(n, mu = c(1,0), Sigma = Sigma) # classe bleue
  x_1 <- MASS::mvrnorm(n, mu = c(0,1), Sigma = Sigma) # classe orange
# matrice des observations  
  x <- rbind(x_0, x_1)
```

Nous voyons que les données de la catégorie "bleue" sont séparées des données de la catégorie "orange".

```{r, include=FALSE}
  y <- rep(c("blue","orange"), each = n)
```


```{r cm4_gauss_class, echo = FALSE}
  plot(x, pch = 19, col = y)  
  abline(-3/4, 5/2, lwd = 2, col = "green3") ## trace x_2 = -3/4 + 5/2 x_1
```

Les erreurs de classification peuvent être représentée par la matrice de confusion suivante
```{r cm4_table}
# matrice de confusion (predictions dures en ligne)
  table(2*x[,2] > -3/2 + 5*x[,1], y == "orange")
```

L'erreur de classification peut être estimée de la manière suivante
```{r}
# 1 - accuracy
  mean((2*x[,2] > -3/2 + 5*x[,1]) != (y == "orange"))
```
et l'erreur moyenne logloss peut être calculée de la  manière suivante
```{r}
#log-loss optimal (théorique) 
proba <- 1/(1 + exp(5/3*x[,1] - 2/3*x[,2] - 1/2))
- mean((y=="orange")*log(proba) + (y=="bleu")*log(1-proba))
```

Envisageons maintenant l'utilisation d'un modèle paramétrique de complexité élevée. Pour cela, nous anticipons un peu le cours suivant en utilisant un réseau de neurones. Le paramètre _size_ controle la complexité du modèle, qui comportent 201 paramètres.


```{r cm4_nnet}
mod <- nnet::nnet(x, 
                  as.numeric(y == "orange"), 
                  size = 50, 
                  maxit = 500, 
                  decay = 0.001,
                  trace = FALSE, 
                  entropy = TRUE)
```

Pour ce réseau neuronal, nous pouvons tracer la frontière de prédiction obtenue et la comparer à la frontière optimale (ligne verte). La frontière calculée par le modèle sur-paramétré est extrêmement déchiquetée, et sensible à la variance des données. Il est fort possible que les ilôts mis en evidence par la méthode neuronale soient des artifices non-reproductibles, et qu'ils ne sont pas représentatifs d'un nouveau jeu de données. La frontière très irrégulière illustre le phénomène de sur-apprentissage. 


```{r cm4_nnet_overfit, echo = FALSE}
x_coord <- seq(min(x[,1]), max(x[,1]), length = 100)
y_coord <- seq(min(x[,2]), max(x[,2]), length = 100)

matrice_test <- cbind(rep(x_coord, length = 100), 
                      rep(y_coord, each = 100))

pred <-  predict(mod, matrice_test)

#image(x.coord, y.coord,  matrix(pred > 0.5, nrow = 100), 
#      col = grey.colors(2), main = "Modèle neuronal sur-paramétré")
#plot(x, pch = 19, cex = 1.2, col = y)
#image(x.coord, y.coord,  matrix(pred, nrow = 100), 
#      col = grey.colors(2), main = "Modèle neuronal sur-paramétré")
#points(x, pch = 19, cex = 1.2, col = y)

plot(x, pch = 19, cex = 1.2, col = y, main = "Modèle neuronal sur-paramétré")
#abline(-3/2, 5/2, lwd = 4, col = "yellow") ## trace x_2 = -3/2 + 5/2 x_1
contour(x_coord, y_coord,
        matrix(pred, nrow = 100), col = "green3", 
        levels = 0.5, lwd = 2, add = T)
```

Un affichage interactif permettant de zoomer l'image nous montre les détails d'un paysage chahuté.

```{r cm4_plotly, echo = FALSE}
library(plotly)
plot_ly(z = matrix(pred, nrow = 100), type = "contour")
```


Lorsque l'on cherche à évaluer le modèle sur l'ensemble de données d'apprentissage, les performances semblent bien supérieures à celles de la machine optimale linéaire. 

```{r}
  pred <- predict(mod)
  
  # accuracy
  mean( (pred > 0.5) == (y == "orange"))
  
  #logloss
  pred[pred == 1] <-  0.999999
  pred[pred == 0] <-  0.000001
  - mean((y=="orange")*log(pred) + (y=="bleu")*log(1-pred))
##overfit!
```


Toutefois, en considérant un ensemble de données tests indépendant de l'ensemble d'apprentissage, nous voyons que les performances du modèle sont nettement inférieures à celles du modèle optimal.

```{r, include = FALSE}
  x_test0 <- MASS::mvrnorm(n, mu = c(1,0), Sigma = Sigma)
  x_test1 <- MASS::mvrnorm(n, mu = c(0,1), Sigma = Sigma) 
  x_test <- rbind(x_test0, x_test1)
  
  pred <- predict(mod, x_test)
```

```{r}
  # accuracy
    mean( (pred > 0.5) == (y == "orange"))

  #logloss
    pred[pred == 1] <-  0.9999999999999
    pred[pred == 0] <-  0.0000000000001
    
  - mean((y=="orange")*log(pred) + (y=="bleu")*log(1-pred))
```


## Principales notions à retenir et à savoir définir


Dans cette séance, nous avons vu les points suivants.

* En apprentissage supervisé, une machine probabiliste est définie par une fonction calculant des probabilités pour chaque catégorie ($y = k$) et toute observation ${\bf x}$. On parle de classifieur probabiliste ou de modèle de prédiction.    

* Un algorithme d'apprentissage supervisé est un algorithme qui ajuste le modèle de prédiction en  minimisant une fonction de perte calculée à partir des données observées. Les fonctions de perte les plus utilisées sont l'erreur de classification moyenne (_accuracy_) et la perte d'information (_logloss_). 

* Le classifieur probabiliste optimal est obtenu en calculant les probabilités conditionnelles des catégories sachant les observations. Cela suppose un échantillon de taille infinie (il peut parfois être obtenu théoriquement en utilisant la formule de Bayes). Le classifieur déterministe optimal choisit la catégorie ayant la plus grande probabilité conditionnelle sachant ${\bf x}$.

* Le phénomène de surapprentissage apparaît lorsque la taille de l'échantillon d'apprentissage est petite devant le nombre de paramètres du modèle de prédiction. 

* Pour évaluer correctement un algorithme d'apprentissage, on calcule la fonction de perte sur un ensemble de données n'ayant pas servi lors de la phase apprentissage.   
