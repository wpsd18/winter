# Théorie de la décision

## Introduction et objectifs (à compléter)

* Formule de Bayes 
* Erreur de classification moyenne (_accuracy_)
* Perte d'information (_logloss_)
* Classifieur probabiliste optimal 

## Quelques notions probabilistes supplémentaires

Nous définissons les notions d'espérance conditionnelle, et de formule de Bayes, ainsi que les formules de conditionnement. Ces notions élémentaires seront très utiles pour la bonne compréhension des méthodes employées par la suite.    

### Espérance conditionnelle

On considère un couple formé d'un vecteur et d'une variable aléatoire, $({\bf x},y)$, associant un vecteur de mesures multidimensionnelles, ${\bf x}$, à une variable discrète, $y$. La variable $y$ représente typiquement une catégorie pour ce vecteur ($y \in \{1,\dots,K\}$). Par exemple, ${\bf x}$ peut être une image et $y$ un contenu pour cette image (un chat, un chien ou une autre chose). 

Nous souhaitons calculer l'espérance d'une variable aléatoire à valeurs réelles $z$ définie comme une fonction du couple précédent

$$
z = f({\bf x}, y) \, .
$$

**Définition.** L'*espérance conditionnelle* de $z$ sachant ${\bf x}$ se calcule en toute généralité de la manière suivante
$$
\mathbb{E}[z | {\bf x} ]  = \int z p(z | {\bf x}) dz = \int f({\bf x},y) p( y| {\bf x}) dy \, .
$$

Lorsque la variable $y$ est discrète et prend ses valeurs parmi $K$ modalités, l'intégrale peut être remplacée par une somme

$$
\mathbb{E}[z | {\bf x} ]  = \sum_{k = 1}^K f({\bf x},k) p( y = k| {\bf x}) \, .
$$

**Formule de conditionnement.** Pour obtenir l'espérance de $z$, il suffit de sommer sur toutes les valeurs possibles de ${\bf x}$

$$
\mathbb{E}[z]  =  \int  \mathbb{E}[z | {\bf x} ] p({\bf x}) d {\bf x} .
$$

Pour comprendre la logique de cette formule, on pourra retenir l'exemple suivant. Supposons qu'un cours  comporte deux groupes, ayant respectivement 12 et 28 inscrits. À l'examen, la moyenne du premier groupe est égale à 12 et la moyenne du second groupe est égale à 14. On se demande alors quelle est la moyenne globale des inscrits. Une mauvaise réponse (égale à 13) consiste à faire la moyenne des deux notes. La bonne réponse prend en compte la probabilté de se trouver dans chaque groupe et elle est égale à $12 \times 12/40 + 13 \times 28/40 = 13.4$.

### Formule de Bayes

Sous les mêmes conditions que dans la section précédente, supposons que $y$ est une variable discrète prenant $K$ valeurs. La *formule de Bayes* permet de calculer la probabilité conditionnelle $p(y = \ell|{\bf x})$  à partir de la loi conditionnelle du vecteur ${\bf x}$ sachant la valeur  $y = \ell$. La formule s'écrit de la manière suivante 

$$
p(y = \ell|{\bf x}) = \frac{p({\bf x} | y = \ell ) p(y = \ell)}{p({\bf x})} = \frac{p({\bf x} | y = \ell ) p(y = \ell)}{\sum_{k = 1}^K p({\bf x} | y = k ) p(y = k) } \, , \quad \ell = 1, \dots, K.
$$


## Fonctions d'évaluation de classification

### Classification dure ou probabiliste (douce)

Les fonctions d'évaluation de classification, aussi appelées _fonctions de perte_ (loss function), permettent de mesurer la pertinence des classements effectués par des méthodes automatiques (appelées *machines*). Les fonctions d'évaluation sont essentielles pour définir les algorithmes d'apprentissage automatique et pour évaluer les performances des "machines" apprennantes. Dans ce cours, les algorithmes d'apprentissage supervisés sont essentiellement des algorithmes de minimisation d'une fonction de perte. Il existe de nombreuses fonctions de perte. Dans les algorithmes classiques, les fonctions les plus souvent utilisées sont les fonctions _accuracy_ et _logloss_ dont nous verrons les définitions un peu plus loin. 


En apprentissage supervisé, on distingue deux types d'évaluation selon que l'on s'intéresse à une classification *dure* (_hard classifier_) ou *probabiliste* (_soft classifier_).  On parle de *classification dure* lorsque l'on construit une fonction (ou "machine") classant un vecteur de mesures observées ${\bf x}$ dans une catégorie choisie parmi $K$ possibles

$$
 { \bf x }  \mapsto c({ \bf x })  \in \{ 1, \dots, K\} \, .
$$
Une fonction de perte est une fonction $L(c({\bf x}),y)$ donnant une valeur à l'erreur commise lorsque la machine remplace $y$ par $c({\bf x})$. 

On parle de *classification probabiliste* (ou douce) lorsque l'on construit une fonction calculant les probabilités de chacune des $K$ classes possibles


$$
 { \bf x }  \mapsto q({ \bf x }) = (q_1({ \bf x }), \dots, q_K({ \bf x }))   \in [0,1]^K \, , \quad {\rm t.q.} \quad \sum_{k = 1} q_k({ \bf x }) = 1 \, .
$$


Lorsque le classifieur est probabiliste, il est courant de convertir la valeur $y$ comprise entre 1 et $K$ en un vecteur unitaire de longueur $K$, dont les coordonnées sont toutes nulles sauf celle qui la $y$-ème. Cette dernière est égale à 1. Une fonction de perte est alors une fonction $L(q({\bf x}), y)$ mesurant l'erreur (ou la perte d'information) induite par l'approximation du vecteur unitaire par $q({\bf x})$.  


### Erreur de classification (01-loss)

La plus simple des fonctions de perte consiste à compter les erreurs de classification commises par une machine dure. Nous verrons que la machine dure minimisant cette erreur s'appuie directement sur une machine probabiliste. Les machines considérées par la suite seront donc des machines probabilistes.

L'erreur de classification d'une machine dure, $c({\bf x})$, est définie par

$$
 L(c({\bf x}), y) = \left\{  \begin{array}{l} 1  & {\rm si~} c({\bf x}) \neq y, \\ 
0  & {\rm sinon.}
\end{array} \right. 
$$

L'*erreur moyenne de classification* d'une machine dure définie par $c({\bf x})$ est égale à


$$
E = \mathbb{E} [ L(c({\bf x}),y) ]  \approx \frac1n \sum_{i = 1}^n \mathbb{1}_{c({\bf x}_i) \neq y_i} \, .
$$

Cette erreur est très simple à calculer. Dans l'échantillon considéré $({\bf x}_i, y_i)_{i=1,\dots,n}$, on compte le nombre de fois que la catégorie apprise ne coïncide pas avec la valeur observée. On divise ensuite le résultat par la taille de l'échantillon $n$. Dans les logiciels d'apprentissage, le nombre moyen de bonnes classifications est appelé _accuracy_. La valeur de la mesure _accuracy_ est égale à $1 - E$.

### Matrice de confusion

Il est courant de s'intéresser plus précisement aux erreurs de classification commises par un classifieur dur. Pour cela, la notion de *matrice de confusion* est utile. Pour simplifier, considérons le cas d'un problème de classification en deux classes, notées $0$ ou $1$ (pour changer un peu). La matrice de confusion est la matrice de taille $2\times 2$ définie de la manière suivante 

|   | y = 0  | y = 1  |
|:---: |:---|:---|
|$c({\bf x}) = 0$  | TN | FN |
|$c({\bf x}) = 1$  | FP | TP |


Considérant la valeur 1 comme une classification positive et la valeur 0 comme une classification négative, FP et FN dénombrent les faux-positifs et les faux-négatifs. TP et TN dénombrent les vrais-positifs et les vrais-négatifs. L'erreur de classification correspond à l'addition des termes hors diagonale, FP + FN.

Des fonctions de perte plus générales que l'erreur de classification simple peuvent tenir compte de la nature des erreurs de classification en pondérant les contributions respectives des grandeurs FP et FN dans le calcul de la perte.


## Machines optimales

## Règle de classification dure optimale

**Résultat.** Considérons la fonction de perte définie par l'erreur de classification (perte 01). Pour cette fonction de perte, la machine dure optimale est donnée par le calcul des probabilités conditionnelles des classes sachant les variables observées

$$
c_{\rm opt}({\bf x})  = \arg\max_k { p(y = k | {\bf x}) } \,.
$$

En clair, la machine optimale choisit parmi les classes $\{1,\dots, K\}$ la classe ayant la plus grande probabilité conditionnelle, $q_k({\bf x}) = p(y = k | {\bf x})$. Nous voyons ainsi que la machine dure optimale s'appuie sur la machine probabiliste $q({\bf x}) = p(.|{\bf x})$. 

**Preuve.** La preuve de ce résultat est simple et repose sur la loi de Bernoulli. Considérons un classifieur dur quelconque, $c({\bf x})$. D'après la formule de conditionnement, nous avons

$$
\mathbb{E} [ L(c({\bf x}),y) ] =  \int \mathbb{E} [ L(c({\bf x}),y) | {\bf x}] p({\bf x}) d{\bf x}  \, .
$$
Pour minimiser cette expression, il suffit de le faire pour tout vecteur ${\bf x}$ à l'intérieur de l'intégrale. Cela revient à minimiser la grandeur

$$
\mathbb{E} [ L(c({\bf x}),y) | {\bf x}] = p(y \neq c({ \bf x}) | {\bf x}) = 1 - p(y = c({ \bf x}) | {\bf x}).
$$
On peut donc choisir $c({\bf x})$ de sorte à maximiser la probabilité $p(y = c({ \bf x}) | {\bf x})$. Pour maximiser cette grandeur, il y a un nombre fini de cas à évaluer. En effet, supposons que $c({\bf x}) = k$, nous avons

$$
p(y = c({ \bf x}) | {\bf x}) =  p(y = k | {\bf x}) \, .
$$
Nous voyons ainsi que le choix optimal peut s'écrire de la manière suivante

$$
c_{\rm opt}({\bf x})  = \arg\max_{k \in [1,K]} { p(y = k | {\bf x}) } \,.
$$

La stratégie de classification optimale consiste à choisir la catégorie $k$ maximisant la probabilité conditionnelle. Cette règle est souvent appelée la _règle de classification de Bayes_, en référence à la formule de Bayes que l'on peut utiliser pour le calcul de la probabilité conditionnelle apparaissant dans la formule. 


### Exemple unidimensionel

Considérons un exemple unidimensionel dans lequel des variables positives appartiennent à deux catégories équiprobables, numérotées $y = 0$ et $y = 1$. Les variables sont créées selon un modèle génératif de loi exponentielle, dont le paramètre est connu et spécifique à la classe des variables ($\lambda_0$ ou $\lambda_1$ selon que $x$ appartient à la catégorie $0$ et $1$). Pour tout $x>0$, les densités des lois conditionnelles sont données par les formules suivantes
$$
p(x|y = 0) = \lambda_0 \exp(-\lambda_0 x )  
$$
et 
$$
p(x|y = 1) = \lambda_1 \exp(-\lambda_1 x ) \, .
$$
Dans ce cas, nous pouvons appliquer la formule de Bayes pour définir le classifieur optimal. Puisque la variable de classe prend les valeurs $0$ ou $1$, il suffit de calculer la probabilité conditionnelle $p(y = 1 |x)$. La probabilité $p(y = 0 |x)$ s'obtient par complémentaire, $p(y = 0 |x) = 1 - p(y = 1 |x)$.

D'après la formule de Bayes, nous avons

$$
p(y = 1 |x) = \frac{p(x|y=1) p(y = 1)}{p(x|y=0) p(y = 0) + p(x|y=1) p(y = 1)}\,.
$$

Puisque les classes sont équiprobables, nous pouvons supposer que $p(y=1) = p(y=0)$. En remplaçant les densités par leur expression respective, nous obtenons
$$
p(y = 1 |x) = \frac{1}{1 + \exp(-(\lambda_0 - \lambda_1)x - \log(\lambda_1/\lambda_0))}\,.
$$

la règle de classification optimale consiste à classer $x$ en dans la catégorie 1 si 
$$
p(y = 1 |x) > p(y = 0 |x) \, ,
$$
c'est à dire, si $p(y = 1 |x) > 1/2$. Cette condition est équivalente à choisir la catégorie 1 lorsque la variable $x$ est supérieure à un seuil $s$, décrit ci-dessous

$$
x > s = \frac{\log(\lambda_0) - \log(\lambda_1)}{\lambda_0 - \lambda_1} .
$$
La valeur $x = s$ est appelé la _frontière_ de décision.

### Simulation

L'exemple précédent peut être simulé très facilement et nous renseigner sur la borne inférieure de l'erreur moyenne de classification. Pour des valeurs $\lambda_0 = 1/10$ et $\lambda_1 = 1/2$, la frontière de décision est $x = 4.02$ ($\log(x) = 1.39$). L'histogramme des valeurs de $\log(x)$ est séparé par le seuil de décision $\log(x) = 1.39$.

```{r}
n <- 1000
y <- rep(0:1, each = n)
x <- c(rexp(n, rate = .1), rexp(n, rate = .5))
hist(log(x), prob = TRUE)
s <- (log(0.1) - log(0.5))/(0.1 - 0.5)
points(log(s), 10, type = 'h', lwd = 3, col = 2)
```

La valeur de l'erreur pour le classifieur optimal n'est pas égale à zéro. Elle peut être estimée assez précisement de la manière suivante  

```{r}
# Erreur de classification
mean(y != (x<s))
```



## Classification probabiliste optimale

Nous venons de voir que la machine optimale au sens de l'erreur de classification moyenne s'appuie sur un classifieur probabiliste $q_k({\bf x}) = p(y = k | {\bf x})$.

$$
c_{\rm opt}({\bf x})  = \arg\max_k { p(y = k | {\bf x}) } \,.
$$

La plupart des algorithmes d'apprentissage recherchent donc directement un classifieur probabiliste.  Toutefois optimiser l'erreur de classification n'est pas toujours simple numériquement, et d'autres critères ont été proposés. Les plus courant sont l'entropie-croisée, aussi appelée _logloss_ ou tout simplement _entropie_, et l'erreur quadratique moyenne, aussi appelée erreur $L^2$.   


### Erreur log-loss (entropie croisée)

Une fonction d'erreur fréquemment considérée par les algorithmes d'apprentissage probabiliste est la fonction de perte  suivante

$$
L(y , q({\bf x})) = - \log q_y({\bf x}) \, .
$$
Pour une raison évidente, cette erreur est appelée _log-loss_. Lorsque la variable $y$ est binaire, la fonction s'écrit aussi 

$$
L(y , q({\bf x})) = - y \log q({\bf x}) - (1-y)\log (1 - q({\bf x})) \, ,
$$
où $q({\bf x})$ s'identifie à la probabilité de la classe 1.

Nous allons montrer que minimiser l'erreur moyenne log-loss revient à minimiser l'information perdue lorsque l'on approche la loi cible $p(y|{\bf x})$ par une machine probabiliste $q({\bf x})$. L'erreur moyenne associée à la fonction de perte log-loss est égale à

$$
\mathbb{E}[L(y , q({\bf x}))] =  \int \mathbb{E}[L(y , q({\bf x})) | {\bf x}] p({\bf x}) d{\bf x} \, .
$$
Ainsi pour minimiser la fonction de perte, il suffit de trouver la machine probabiliste $q({\bf x})$ minimisant l'espérance conditionnelle

$$
\mathbb{E}[L(y , q({\bf x})) | {\bf x} ] = - \sum_{k = 1}^K p(y = k|{\bf x}) \log q_k({\bf x}) \, .
$$

La quantité qui apparaît dans le terme de droite de cette expression est l'_entropie croisée_ des lois $p(y = k|{\bf x})$ et $q_k({\bf x})$, définies sur l'ensemble de classes $\{1,\dots,K\}$

$$
h(p_{\sf y|{\bf x}} , q({\bf x}) ) = - \sum_{k = 1}^K p(y = k|{\bf x}) \log q_k({\bf x}).
$$

Nous pouvons comparer cette valeur à l'entropie de la loi conditionnelle $p(y = k|{\bf x})$

$$
h(p_{\sf y|{\bf x}} ) = - \sum_{k = 1}^K p(y = k|{\bf x}) \log p(y = k|{\bf x}).
$$

La différence des deux grandeurs est égale à la divergence de Kullback-Lieber

$$
D_{KL}(  p_{\sf y|{\bf x}} \| q({\bf x})  )  =   h(p_{\sf y|{\bf x}} , q({\bf x}) ) - h(p_{\sf y|{\bf x}} )  \, . 
$$

Puisque la divergence de Kullback-Lieber est toujours positive, nous obtenons que 

$$
h(p_{\sf y|{\bf x}}) = h(p_{\sf y|{\bf x}} , p_{\sf y|{\bf x}})  \leq  h(p_{\sf y|{\bf x}} , q({\bf x})) .
$$
Ainsi, l'erreur moyenne logloss est minimale si et seulement, pour tout $k \in 1,\dots, K$,

$$
q_k({\bf x}) = p(y = k|{\bf x}).
$$

Minimiser l'erreur moyenne log-loss revient à minimiser l'information perdue lorsque l'on approche la loi cible $p(y|{\bf x})$ par une machine probabiliste $q({\bf x})$. Ce résultat justifie le critère empirique utilisé par les algorithmes d'apprentissage. En pratique, l'erreur log-loss est calculée empiriquement

$$
{\rm logloss} \approx - \frac1n \sum_{i=1}^n \log( q_{y_i}({ \bf x}_i) )  
$$

Lorsque la classification est binaire, cela revient à calculer

$$
{\rm logloss} \approx - \frac1n \sum_{i=1}^n  y_i \log q({\bf x}_i)   + (1 -y_i) \log(1 - q({ \bf x}_i) ) . 
$$


### Erreur quadratique (reportée)

Une seconde fonction d'erreur fréquemment considérée par les algorithmes d'apprentissage probabiliste est la fonction de perte suivante

$$
L(y , q({\bf x})) = ( y - q_y({\bf x}) )^2 \, .
$$
Pour une raison évidente, cette erreur est appelée perte quadratique ou $L^2$. 

Pour simplifier les notations, nous nous plaçons dans le cas d'une classification binaire. Ce cas est aussi le plus fréquemment rencontré dans les exemples de ce cours. Lorsque la variable $y$ est binaire, la fonction s'écrit tout simplement $L(y , q({\bf x})) = ( y - q({\bf x}) )^2$, où $q({\bf x})$ s'identifie à la probabilité de la classe 1. Pour minimiser la fonction de perte quadratique, il suffit de trouver la machine probabiliste $q({\bf x})$ minimisant l'espérance conditionnelle suivante

$$
\mathbb{E}[L(y , q({\bf x})) | {\bf x} ] = \mathbb{E}[(y - q({\bf x}))^2 | {\bf x} ]  \, .
$$

Faisons apparaître la probabilité conditionnelle de la classe 1 $p(y = 1 | {\bf x})$. Nous avons 

$$
\mathbb{E}[(y - q({\bf x}))^2 | {\bf x} ]  =  \mathbb{E}[(y - p(y = 1 | {\bf x})) - (q({\bf x}) - p(y = 1 | {\bf x})) )^2 | {\bf x} ]   \, .
$$

En developpant le carré, le terme du double produit est nul. Nous avons 

$$
\mathbb{E}[(y - q({\bf x}))^2 | {\bf x} ]  =  \mathbb{E}[(y - p(y = 1 | {\bf x}))^2 | {\bf x} ]  + C({\bf x})^2  \, ,
$$
et, puisque le terme de droite est toujours positif, 

$$
\mathbb{E}[(y - q({\bf x}))^2 | {\bf x} ]  \geq  \mathbb{E}[(y - p(y = 1 | {\bf x}))^2 | {\bf x} ]  \, .
$$
Cela démontre que, pour l'erreur quadratique moyenne, la machine probabiliste optimale correspond à 

$$
q({\bf x}) =  p(y = 1 | {\bf x}) \, .
$$


Minimiser l'erreur quadratique moyenne revient donc à minimiser l'erreur log-loss. Ce résultat justifie l'utilisation fréquence de l'erreur quadratique moyenne par les algorithmes d'apprentissage. En pratique, cette erreur est calculée à partir des données empiriques de la manière suivante

$$
{\rm EQM} \approx  \frac1n \sum_{i=1}^n (y_i - q_{y_i}({\bf x}_i))^2  
$$

Lorsque la classification est binaire, cela revient à calculer

$$
{\rm EQM} \approx  \frac1n \sum_{i=1}^n (y_i - q({\bf x}_i))^2 . 
$$


## Erreur d'apprentissage et erreur de prédiction

L'apprentissage automatique concerne essentiellement des machines (ou prédicteurs) probabilistes. Elle  
consiste en la minimisation d'un critère de perte à partir d'observations répétées du couple classe/variables $(y_i,{\bf x}_i)$, $i = 1, \dots, n$, supposées indépendantes les uns des autres.   

Un algorithme d'apprentissage peut prendre des formes très diverses. De nombreux algorithmes s'appuient sur la miminisation numérique des erreurs empiriques _log loss_ ou  _quadratique_, mais ce n'est pas le cas de toutes les méthodes. En général, un algorithme correspond à la construction d'une machine probabiliste, représentée par une fonction, $\hat q({ \bf x})$, telle que

$$
\hat q({ \bf x}) = \arg\min_q \frac1n \sum_{i = 1}^n L(y_i, q({\bf x}_i)) \,.
$$

Nous appelerons parfois telle machine probabiliste un _modèle de prédiction_.


### Modèles paramétriques 

Il y a deux grandes écoles de pensée pour la construction de _modèle de prédiction_ : les approches paramétriques et les approches non-paramétriques. Les premières regroupent des approches telles que les 
modèles gaussiens, la regression logistique, les réseaux de neurones, etc. Les secondes regroupent des approches telles que les méthodes de plus proches voisins, de distances, les méthodes à noyaux, les machines à vecteurs de support, etc. Ce cours met plutôt l'accent sur les méthodes paramétriques, tout en mentionnnant les approches non-paramétriques.

Les méthodes paramétriques utilisent un ensemble de valeurs numériques, $\theta$, pour paramétrer le modèle de prédiction. Elles supposent que le modèle est une fonction des paramètres 

$$
q({\bf x}) = q({\bf x}, \theta) \,. 
$$
L'algorithme d'apprentissage consiste alors à optimiser le paramètre $\theta$. En effet la fonction d'évaluation devient-elle même une fonction de $\theta$

$$
L(\theta) = \frac1n \sum_{i = 1}^n L(y_i, q({\bf x}_i, \theta)) \,.
$$
Le problème se résume à trouver l'ensemble de paramètres $\hat \theta$ minimisant la fonction $L(\theta)$

$$
\hat \theta = \arg\min_\theta L(\theta) \,.
$$
Le _modèle de prédiction ajusté_ sera alors obtenu en injectant la valeur calculée, $\hat \theta$, dans le modèle de prédiction

$$
\hat q({\bf x}) = q({\bf x}, \hat \theta) \, .
$$ 

Si la fonction de perte et le modèle de prédiction sont suffisament réguliers (différentiables, par exemple), il est possible d'utiliser des algorithmes d'optimisation tels que l'algorithme du gradient ou la méthode de Newton pour trouver les minima de la fonction de perte. 


Pour illustrer la démarche d'optimisation, considérons les données unidimensionnelles générées par des lois exponentielles pour une classification binaire (section précédente), et définissons le modèle de prédiction suivant 

$$
q(x, \theta_1, \theta_2) = \frac{1}{ 1 + \exp(-\theta_1x-\theta_2) } \, .
$$

Pour ce problème, nous pouvons créer une fonction R calculant $L(\theta)$ en nous appuyant sur la perte logarithmique _log-loss_. Cela peut se faire  de la manière suivante 

```{r}
L <- function(th){
  # critère de décision \theta_1x + \theta_2
  z <- th[1]*x + th[2]
  # lien sigmoide 
  prob <- 1/(1 + exp(-z))
  # renvoie la log-loss
  return(- mean(y*log(prob) + (1-y)*log(1-prob)))
}
```

Il est possible de minimiser cette fonction en utilisant la commande _optim()_ de R (exemple de programmation fonctionnelle). Pour cela nous utilisons un algorithme numérique classique, appelé méthode "BFGS"

```{r}
# minimisation de la fonction L(theta)
# condition initiale th1 = -1, th2 = 2
  obj <- optim(c(-1,2), fn = L, method = "BFGS")
# solution
  obj$par
```

 Le calcul théorique de la probabilité $p(y = 1 |x)$ a été effectué dans la section précédente. Il nous indique que la solution optimale est $\theta_1 = - 0.4$ et $\theta_2 = \log(5) = 1.60$. Les valeurs numériques sont donc proches des valeurs obtenues théoriquement en considérant un échantillon de taille $n = \infty$.

### Méthodes non-paramétriques (à compléter)

Les méthodes non-paramétriques s'appuie sur un nombre très restreint de paramètres et tentent d'estimer les probabilités conditionnelles des classes par des approches locales. La méthode des _k plus proches voisins_ est une illustration directe de telles approches. 

![Méthode des k plus proches voisins : Les variables observées pour deux classes "bleues" et "rouges" correspondent aux carrés et aux triangles. On détermine la probabilité de la classe au point vert par un sondage des points voisins de ce point testé. La méthode s'avère inefficace en grande dimension à cause du fléau de la dimension.](./figures/Knn.png)

Dans la méthode des plus proches voisins, on décide d'une distance entre les variables. On détermine alors la probabilité pour qu'un vecteur ${\bf x}$ appartiennent à une classe donnée en sondant les $k$ vecteurs de l'échantillon les proches de ${\bf x}$. La méthode de sondage peut parfois introduire des pondérations des votes en fonction de la distance au vecteur cible ${\bf x}$. Cette pondération est souvent appelée noyau et plusieurs formes de noyaux peuvent être proposées.

### Malédiction de la dimension (à développer)

Exemple du cube de coté $a$ nécessaire pour occuper 10$\%$ du volume total dans le volume unité ($a \to 1$).

### Ensemble test

Les modèles paramétriques peuvent nécéssiter un grand nombre de paramètres. Nous verrons dans la suite que ce nombre peut être gigantesque pour les réseaux de neurones multicouches.

Dans ce cas, la taille de l'échantillon peut être insuffisante pour correctement évaluer la valeur moyenne de la fonction d'évaluation et le phénomène de sur-apprentissage ou sur-paramétrage (overfitting) apparait. 

![Illustration du phénomène de sur-apprentissage pour un problème de classification (rouge/bleu). La fontière noire correspond à la frontière de décision optimale. La frontière verte correspond à la frontière calculée par un modèle sur-paramétré. Source de l'image: wikipedia](./figures/Overfitting.png)



Revenons en dimension $D=1$ et supposons que l'on dispose de $n$ points répartis en $2$ classes (0 ou 1). Supposons que l'on souhaite ajuster le modèle unidimensionnel $q(x, \theta)$ décrit ci-dessous, possédant $2n$ paramètres 

$$
q(x, \theta) = \sum_{i = 1}^n \theta_{1,i} \delta_{\theta_{2,i} } (x) \,  .
$$

Pour ajuster ce modèle, nul besoin d'algorithme numérique. La solution évidente consiste à poser $\hat\theta_{1,i} = y_i$ et $\hat\theta_{2,i} = x_i$, pour tout $i$ de 1 à $n$. Les erreurs de classification, d'entropie ou de moindres carrés sont alors égales à zéro, la valeur minimale absolue.

Cette machine (inutile) montre qu'il est toujours possible de construire un algorithme obtenant une erreur nulle sur l'ensemble d'apprentissage en considèrant suffisamment de paramètres dans un modèle. Certains modèles, dits flexibles ou peu contraints, pourront montrer des problèmes similaires.

Sauf si l'ensemble d'apprentissage est infiniment grand, nous comprenons qu'il faut être vigilant lors de l'évaluation un prédicteur sur cet ensemble. 

La suite nous montre que la machine inutile n'a effectivement aucun intérêt prédictif.Nous avons vu précédement que la grandeur cible, que l'on cherche à minimiser, est l'erreur théorique suivante

$$
E = \mathbb{E}[ L(y, q(x) )] \,.
$$

Pour la machine que nous venons de construire, l'erreur est  théoriquement calculable.   Le résultat est égal à la probabilité $p(y = 1)$ pour la perte quadratique ou pour l'erreur de classification binaire. Il est égal $+\infty$ pour l'erreur d'entropie. Nous voyons que l'erreur moyenne n'est pas égale à zéro (sauf si $p(y = 0) = 1$). Si les classes sont équireprésentées, les performances théoriques de l'algorithme d'apprentissage sont très mauvaises. Nous pouvons aussi vérifier ce résultat avec une erreur calculée empiriquement sur un échantillon test indépendant de l'échantillon d'apprentissage. 

En général, on réservera un sous echantillon pour minimiser l'erreur d'apprentissage et un second sous-echantillon, composé de données non-utilisées pendant l'apprentissage, pour évaluer l'erreur cible, $E$. Selon la taille du problème, on peut se permettre de réserver un pourcentage plus ou moins élevé de données pour l'ensemble de test (20$\%$ semble être un compromis moderne acceptable). Il sera alors important de faire varier la complexité du modèle et de choisir le modèle produisant la plus petite erreur sur l'ensemble test.  

![Erreurs d'apprentissage vs erreur de test en fonction de la complexité d'un modèle. L'erreur calculée sur l'ensemble test (courbe rouge) est en général plus élevée que l'erreur calculée sur l'ensemble d'apprentissage. Le meilleur choix de modèle correspond au minimum de l'erreur calculée sur l'ensemble test. Source de l'image: wikipedia.](./figures/Overfitting2_1.png)


```{r fig.width=2, fig.height=2, echo=FALSE, fig.cap="Erreurs d'apprentissage vs erreur de test en fonction de la complexité d'un modèle. L'erreur calculée sur l'ensemble test (courbe rouge) est en général plus élevée que l'erreur calculée sur l'ensemble d'apprentissage. Le meilleur choix de modèle correspond au minimum de l'erreur calculée sur l'ensemble test. Source de l'image: wikipedia."}
#library(png)
#library(grid)
#  img <- readPNG("bookdown/figures/Overfitting3.png")
#  grid.raster(img)
```


### Exemple (séparation linéaire)

On considère un vecteur de caractéristiques ${\bf x}$ en dimension $D = 2$. On suppose que les données se répartissent en deux classes, $y = 0$ et $y = 1$. On suppose de plus que les données ont été générées en proportions égales dans chacune des deux classes.

Nous étudions un modèle génératif reposant sur des lois conditionnelles gaussiennes de moyenne respectives $\mu_0 = (1,0)$ pour la classe 0 et $\mu_1 = (0,1)$ pour la classe 1. La matrice de covariance $\Sigma$ est identique pour les deux classes et égale à 

$$
\Sigma = \left( \begin{array}{cc} 1 & 1 \\
1 & 4 \\
\end{array} \right) .
$$

Le modèle de génération de données se décrit donc de la manière suivante

$$
p( {\bf x}  | y = k ) = N( {\bf x}  | \mu_k, \Sigma ) \, , \quad  \forall k = 0, 1, \quad {\bf x} \in \mathbb{R}^2.
$$

Nous supposons les moyennes et la matrice $\Sigma$ connus (ou estimés sans erreur). Pour ce modèle génératif, comme pour l'exemple exponentiel vu précédemment, nous pouvons déterminer exactement la probabilité conditionnelle de chaque classe. le calcul conduit à

$$
p( y = 1 | {\bf x} ) = \frac{1}{1 + \exp( - {\bf w}^T  ( {\bf x} - {\bf x}_0) )}  \, ,
$$

où
$$
{\bf w} = \Sigma^{-1} (\mu_1 - \mu_0) 
$$
et
$$
{\bf x}_0 = (\mu_0 + \mu_1)/2 .
$$

Avec les valeurs choisies pour exemple, la frontière de décision optimale est la droite d'équation $x_2 = -3/2 + 5 x_1/2$. Nous pouvons visualiser la séparation des classes à l'aide d'une simulation comportant 200 données

```{r, include=FALSE}
library(magrittr)
```


```{r}
  Sigma <- c(1,1,1, 4) %>% matrix(nrow=2)
  n <- 100
  x_0 <- MASS::mvrnorm(n, mu = c(1,0), Sigma = Sigma)
  x_1 <- MASS::mvrnorm(n, mu = c(0,1), Sigma = Sigma) 
  x <- rbind(x_0, x_1)
```


```{r, include=FALSE}
  y <- rep(c("blue", "orange"), each = n)
```


```{r, echo = FALSE}
  plot(x, pch = 19, col = y)  
  abline(-3/2, 5/2, lwd = 2, col = "green3") ## trace x_2 = -3/2 + 5/2 x_1
```

Les erreurs de classification peuvent être représentée par la matrice de confusion 
```{r}
# matrice de confusion (predictions dures en ligne)
table(2*x[,2] > -3 + 5*x[,1], y == "orange")
```

L'erreur de classification est estimée de la manière suivante
```{r}
# 1 - accuracy
1 - mean( (2*x[,2] > -3 + 5*x[,1]) == (y == "orange") )
```
et l'erreur moyenne logloss est calculée de la  manière suivante
```{r}
#log-loss optimal (théorique) 
proba <- 1/(1 + exp( 5/3*x[,1] - 2/3*x[,2] - 1/2))
- mean((y=="orange")*log(proba) + (y=="bleu")*log(1-proba))
```

Envisageons maintenant l'utilisation d'un modèle paramétrique de complexité élevée. Pour cela, nous anticipons un peu le cours suivant en utilisant un réseau de neurones. Le paramètre _size_ controle la complexité du modèle, qui comportent 201 paramètres.


```{r}
mod <- nnet::nnet(x, 
                  as.numeric(y =="orange"), 
                  size = 50, 
                  maxit = 500, 
                  decay = 0.001,
                  trace = FALSE, 
                  entropy = TRUE)
```

Pour ce réseau neuronal, nous pouvons tracer la frontière de prédiction obtenue et la comparer à la frontière optimale (ligne jaune). La frontière calculée par le modèle sur-paramétré est extrêment déchiquetée, et sensible à la variance des données. Il est fort possible que les ilôts mis en evidence par la méthode neuronale soient des artifices non-reproductibles, et qu'il ne sont pas représentatifs d'un nouveau jeu de données. La frontière très irrégulière illustre le phénomène de sur-apprentissage. 


```{r, echo = FALSE}
x.coord <- seq(min(x[,1]), max(x[,1]), length = 100)
y.coord <- seq(min(x[,2]), max(x[,2]), length = 100)

matrice.test <- cbind(rep(x.coord, length = 100), 
                      rep(y.coord, each = 100))

pred <-  predict(mod, matrice.test)

#image(x.coord, y.coord,  matrix(pred > 0.5, nrow = 100), 
#      col = grey.colors(2), main = "Modèle neuronal sur-paramétré")
#plot(x, pch = 19, cex = 1.2, col = y)
#image(x.coord, y.coord,  matrix(pred, nrow = 100), 
#      col = grey.colors(2), main = "Modèle neuronal sur-paramétré")
#points(x, pch = 19, cex = 1.2, col = y)

plot(x, pch = 19, cex = 1.2, col = y, main = "Modèle neuronal sur-paramétré")
#abline(-3/2, 5/2, lwd = 4, col = "yellow") ## trace x_2 = -3/2 + 5/2 x_1
contour(x.coord,y.coord,
        matrix(pred, nrow = 100), col = "green3", 
        levels = 0.5, lwd = 2, add = T)
```

Un affichage interactif permettant de zoomer l'image nous montre les détails d'un paysage chahuté.

```{r, echo = FALSE}
library(plotly)
plot_ly(z = matrix(pred, nrow = 100), type = "contour")
```


Lorsque l'on cherche à évaluer le modèle sur l'ensemble de données d'apprentissage, les performaces sont bien supérieures à celle de la machine optimale linéaire. 

```{r}
pred <- predict(mod)
# accuracy
mean( (pred > 0.5) == (y == "orange"))
#logloss
pred[pred == 1] <-  0.999999
pred[pred == 0] <-  0.000001
- mean((y=="orange")*log(pred) + (y=="bleu")*log(1-pred))
##overfit!
```


Toutefois, en considérant un ensemble de données indépendantes, nous voyons que les performances du modèle sont nettement inférieures à celle du modèle optimal

```{r, include = FALSE}
  x.test0 <- MASS::mvrnorm(n, mu = c(1,0), Sigma = Sigma)
  x.test1 <- MASS::mvrnorm(n, mu = c(0,1), Sigma = Sigma) 
  x.test <- rbind(x.test0, x.test1)
  
  pred <- predict(mod, x.test)
```

```{r}
# accuracy
mean( (pred > 0.5) == (y == "orange"))
#logloss
pred[pred == 1] <-  0.9999999999999
pred[pred == 0] <-  0.0000000000001
- mean((y=="orange")*log(pred) + (y=="bleu")*log(1-pred))
```



