# Théorie de la décision

## Introduction et objectifs (à compléter)

Deux critères _accuracy_ et _logloss_, discussion de l'optimalité des classifieurs.

## Quelques notions probabilistes (à compléter)

### Espérance conditionnelle

On considère un couple de variable aléatoire $({\bf x},y)$, associant par exemple un vecteur de caractéristiques multidimensionnelles, ${\bf x}$, et une variable discrète, $y$, représentant la classe de ce vecteur (d'autres combinaisons sont possibles). Nous nous intéressons à l'espérance d'une variable aléatoire $z$ définie comme fonction du couple précédent

$$
z = f(y,{\bf x}) \, .
$$
L'espérance conditionnelle de $z$ sachant ${\bf x}$ se calcule en générale de la manière suivante
$$
\mathbb{E}[z | {\bf x} ]  = \int z p(z | {\bf x}) dz = \int f(y,{\bf x}) p( y| {\bf x}) dy .
$$

Lorsque la variable $y$ est discrète et prend ses valeurs parmi $K$ modalités, nous obtenons

$$
\mathbb{E}[z | {\bf x} ]  = \sum_{k = 1}^K f(k,{\bf x}) p( y = k| {\bf x}) \, .
$$
Pour obtenir l'espérance de $z$, il suffit de sommer sur toute les valeurs possibles de ${\bf x}$

$$
\mathbb{E}[z]  =  \int  \mathbb{E}[z | {\bf x} ] p({\bf x}) d {\bf x} .
$$

### Formule de Bayes

Sous les mêmes conditions que dans la section précédente, supposons que $y$ est discrète. La formule de Bayes permet de calculer la probabilité conditionnelle $p(y = k|{\bf x})$  à partir des lois conditionnelles du vecteur ${\bf x}$ : 

$$
p(y = \ell|{\bf x}) = \frac{p({\bf x} | y = \ell ) p(y = \ell)}{\sum_{k = 1}^K p({\bf x} | y = k ) p(y = k) } \, , \quad k = 1, \dots, K.
$$


## Fonctions d'évaluation (loss function)

### Classification dure ou probabiliste

Les fonctions d'évaluation, aussi appelées _fonctions de perte_ (loss function), permettent de mesurer la pertinence des classements effectués par les machines. Ces fonctions sont essentielles pour définir les algorithmes d'apprentissage automatique et pour évaluer les performances des "machines" apprennantes. Les algorithmes d'apprentissage sont essentiellement des algorithmes de minimisation d'une fonction de perte. Dans les logiciels modernes, les fonctions _accuracy_ et _logloss_ sont les plus souvent utilisées. 


On distingue deux types d'objectifs, évaluer une classification dure (_hard classifier_, machine dure) ou douce (_soft classifier_, machine molle). 

On parle de classification dure lorsque l'on réalise une machine (en fait, une fonction) classant un vecteur de caractérisques observées ${\bf x}$ dans l'une des $K$ classes possibles

$$
 { \bf x }  \mapsto c({ \bf x })  \in \{ 1, \dots, K\} \, .
$$

On parle de classification probabiliste (ou molle) lorsque l'on réalise une machine (en fait, une fonction) calculant les probabilités de chacune des $K$ classes possibles


$$
 { \bf x }  \mapsto q({ \bf x }) = (q_1({ \bf x }), \dots, q_K({ \bf x }))   \in [0,1]^K \, , \quad {\rm t.q.} \quad \sum_{k = 1} q_k({ \bf x }) = 1 \, .
$$

Une fonction de perte est une fonction $L(y, c)$ mesurant l'erreur commise  par la machine en approchant $y$ par $c$. Lorsque le classifieur est probabiliste, une fonction de perte est une fonction $L(y, q)$ mesurant la perte d'information induite par l'approximation de la loi de $y$ par $q$.  


### Erreur de classification (01-loss)

La plus simple des fonctions d'évaluation est l'erreur de classification commise en moyenne par une d'une machine dure. Nous verrons que la machine dure optimale pour cette erreur s'appuie directement sur une machine probabiliste. Les machines (ou modèle) d'apprentissage considérées par la suite seront donc des machines probabilistes.

L'erreur de classification d'une machine dure $c({\bf x})$ est définie par

$$
 L(y, c({\bf x})) = \left\{  \begin{array}{l} 1  & {\rm si~} c({\bf x}) \neq y, \\ 
0  & {\rm sinon.}
\end{array} \right. 
$$

L'erreur moyenne de classification d'une machine dure définie par $c({\bf x})$ est égale à


$$
E = \mathbb{E} [ L(y, c({\bf x})) ]  \approx \frac1n \sum_{i = 1}^n L(y_i, c({\bf x}_i))  \, .
$$

Dans les logiciels d'apprentissage, l'erreur moyenne de classification est appelé _accuracy_.

### Matrice de confusion

Il est courant de s'intéresser plus précisement aux erreurs de classification. Pour cela, la matrice de confusion est utile. Considérons le cas d'un problème de classification en deux classes $0$ ou $1$.
La _matrice de confusion_ est la matrice de taille $2\times 2$ définie de la manière suivante 

|  y | 0  | 1  |
|:---: |:---|:---|
|$c(x) = 0$  | TN | FN |
|$c(x) = 1$  | FP | TP |


Considérant la valeur 1 comme une classification positive et la valeur 0 comme une classification positive, FP et FN dénombrent les faux-positifs et les faux-négatifs. TP et TN dénombrent les vrais-positifs et les vrais-négatifs. Les erreurs de classification correspondent aux termes hors diagonale, FP + FN.

Des fonctions de perte plus générale que l'erreur de classification simple  peuvent tenir compte de la nature des erreurs de classification.


## Machine optimale 

## Règle de classification dure optimale

La machine dure optimale pour l'erreur de classification est donnée par le calcul des probabilités conditionnelles des classes sachant les variables observées ${\bf x}$

$$
c_{\rm opt}({\bf x})  = \arg\max_k { p(y = k | {\bf x}) } \,.
$$
En clair, la machine optimale choisit parmi les classes $\{1,\dots, K\}$ la classe de ayant la plus grande probabilité conditionnelle, $q_k({\bf x}) = p(y = k | {\bf x})$. Nous voyons ainsi que la machine dure optimale s'appuie sur la machine molle $q({\bf x})$. 

La preuve est simple et repose sur la loi de Bernoulli. Considérons un classifieur dur $c({\bf x})$. D'après la formule de conditionnement, nous avons

$$
\mathbb{E} [ L(y, c({\bf x})) ] =  \int \mathbb{E} [ L(y, c({\bf x})) | {\bf x}] p({\bf x}) d{\bf x}  
$$
Pour minimiser cette expression, il suffit de le faire pour toute valeur à l'intérieur de l'intégrale. Cela revient à minimiser la grandeur

$$
\mathbb{E} [ L(y, c({\bf x})) | {\bf x}] = p(y \neq c({ \bf x}) | {\bf x}) = 1 - p(y = c({ \bf x}) | {\bf x}).
$$
On peut donc choisir $c({\bf x})$ de sorte à maximiser la probabilité $p(y = c({ \bf x}) | {\bf x})$. Or nous avons un nombre fini ($K$) de possibilités

$$
p(y = c({ \bf x}) | {\bf x}) =  p(y = k | {\bf x}) \, , \quad {\rm si~} c({ \bf x}) = k , \quad \forall k= 1,\dots, K\, .
$$
Nous avoyons ainsi que 

$$
c_{\rm opt}({\bf x})  = \arg\max_k { p(y = k | {\bf x}) } \,.
$$
Cette stratégie de classification optimale est souvent appelée la _règle de classification de Bayes_. 


### Exemple unidimensionel

Considérons un exemple unidimensionel très simple dans lequel des variables positives appartenant à deux classes équiprobables, numérotées $0$ et $1$, sont crées par un processus génératif de loi exponentielle de paramètres connus ($\lambda_0$ ou $\lambda_1$ selon la classe). Pour tout $x>0$, nous avons
$$
p(x|y = 0) = \lambda_0 \exp(-\lambda_0 x )  
$$
et 
$$
p(x|y = 1) = \lambda_1 \exp(-\lambda_1 x ) .
$$
Dans ce cas, nous pouvons appliquer la formule de Bayes pour définir la machine optimale. Puisque la variable de classe prend les valeurs $0$ ou $1$, il suffit de calculer la probabilité conditionnelle $p(y = 1 |x)$. Nous avons

$$
p(y = 1 |x) = \frac{p(x|y=1) p(y = 1)}{p(x|y=0) p(y = 0) + p(x|y=1) p(y = 1)}\,.
$$

En remplaçant les densités, nous obtenons
$$
p(y = 1 |x) = \frac{1}{1 + \exp(-(\lambda_0 - \lambda_1)x - \log(\lambda_1/\lambda_0))}\,.
$$

la règle optimale consiste à classer $x$ en 1 si $p(y = 1 |x) > 1/2$, c'est à dire, si

$$
x > s = \frac{\log(\lambda_0) - \log(\lambda_1)}{\lambda_0 - \lambda_1} .
$$
La valeur $x = s$ est appelé le _seuil_ ou la _frontière_ de décision.

### Simulation

L'exemple précédent peut être simulé très facilement et nous renseigner sur la borne inférieure de l'erreur moyenne de classification. Pour des valeurs $\lambda_0 = 1/10$ et $\lambda_1 = 1/2$, la frontière de décision est $x = 4.02$ ($\log(x) = 1.39$). L'histogramme des valeurs de $\log(x)$ est séparé par le seuil de décision $\log(x) = 1.39$.

```{r}
n <- 1000
y <- rep(0:1, each = n)
x <- c(rexp(n, rate = .1), rexp(n, rate = .5))
hist(log(x), prob = TRUE)
s <- (log(0.1) - log(0.5))/(0.1 - 0.5)
points(log(s), 10, type = 'h', lwd = 3, col = 2)
```

La valeur de l'erreur pour le classifieur optimal n'est pas égale à zéro. Elle peut être estimée assez précisement de la manière suivante  

```{r}
# Erreur de classification
mean(y != (x<s))
```



## Classification probabiliste optimale

Nous venons de voir que la machine optimale au sens de l'erreur de classification moyenne s'appuie sur un classifieur probabiliste $q_k({\bf x}) = p(y = k | {\bf x})$.

$$
c_{\rm opt}({\bf x})  = \arg\max_k { p(y = k | {\bf x}) } \,.
$$

La plupart des algorithmes d'apprentissage recherchent donc directement un classifieur probabiliste.  Toutefois optimiser l'erreur de classification n'est pas toujours simple numériquement, et d'autres critères ont été proposés. Les plus courant sont l'entropie-croisée, aussi appelée _logloss_ ou tout simplement _entropie_, et l'erreur quadratique moyenne, aussi appelée erreur $L^2$.   


### Erreur log-loss (entropie croisée)

Une fonction d'erreur fréquemment considérée par les algorithmes d'apprentissage probabiliste est la fonction de perte  suivante

$$
L(y , q({\bf x})) = - \log q_y({\bf x}) \, .
$$
Pour une raison évidente, cette erreur est appelée _log-loss_. Lorsque la variable $y$ est binaire, la fonction s'écrit aussi 

$$
L(y , q({\bf x})) = - y \log q({\bf x}) - (1-y)\log (1 - q({\bf x})) \, ,
$$
où $q({\bf x})$ s'identifie à la probabilité de la classe 1.

Nous allons montrer que minimiser l'erreur moyenne log-loss revient à minimiser l'information perdue lorsque l'on approche la loi cible $p(y|{\bf x})$ par une machine probabiliste $q({\bf x})$. L'erreur moyenne associée à la fonction de perte log-loss est égale à

$$
\mathbb{E}[L(y , q({\bf x}))] =  \int \mathbb{E}[L(y , q({\bf x})) | {\bf x}] p({\bf x}) d{\bf x} \, .
$$
Ainsi pour minimiser la fonction de perte, il suffit de trouver la machine probabiliste $q({\bf x})$ minimisant l'espérance conditionnelle

$$
\mathbb{E}[L(y , q({\bf x})) | {\bf x} ] = - \sum_{k = 1}^K p(y = k|{\bf x}) \log q_k({\bf x}) \, .
$$

La quantité qui apparaît dans le terme de droite de cette expression est l'_entropie croisée_ des lois $p(y = k|{\bf x})$ et $q_k({\bf x})$, définies sur l'ensemble de classes $\{1,\dots,K\}$

$$
h(p_{\sf y|{\bf x}} , q({\bf x}) ) = - \sum_{k = 1}^K p(y = k|{\bf x}) \log q_k({\bf x}).
$$

Nous pouvons comparer cette valeur à l'entropie de la loi conditionnelle $p(y = k|{\bf x})$

$$
h(p_{\sf y|{\bf x}} ) = - \sum_{k = 1}^K p(y = k|{\bf x}) \log p(y = k|{\bf x}).
$$

La différence des deux grandeurs est égale à la divergence de Kullback-Lieber

$$
D_{KL}(  p_{\sf y|{\bf x}} \| q({\bf x})  )  =   h(p_{\sf y|{\bf x}} , q({\bf x}) ) - h(p_{\sf y|{\bf x}} )  \, . 
$$

Puisque la divergence de Kullback-Lieber est toujours positive, nous obtenons que 

$$
h(p_{\sf y|{\bf x}}) = h(p_{\sf y|{\bf x}} , p_{\sf y|{\bf x}})  \leq  h(p_{\sf y|{\bf x}} , q({\bf x})) .
$$
Ainsi, l'erreur moyenne logloss est minimale si et seulement, pour tout $k \in 1,\dots, K$,

$$
q_k({\bf x}) = p(y = k|{\bf x}).
$$

Minimiser l'erreur moyenne log-loss revient à minimiser l'information perdue lorsque l'on approche la loi cible $p(y|{\bf x})$ par une machine probabiliste $q({\bf x})$. Ce résultat justifie le critère empirique utilisé par les algorithmes d'apprentissage. En pratique, l'erreur log-loss est calculée empiriquement

$$
{\rm logloss} \approx - \frac1n \sum_{i=1}^n \log( q_{y_i}({ \bf x}_i) )  
$$

Lorsque la classification est binaire, cela revient à calculer

$$
{\rm logloss} \approx - \frac1n \sum_{i=1}^n  y_i \log q({\bf x}_i)   + (1 -y_i) \log(1 - q({ \bf x}_i) ) . 
$$


### Erreur quadratique (reportée)

Une seconde fonction d'erreur fréquemment considérée par les algorithmes d'apprentissage probabiliste est la fonction de perte suivante

$$
L(y , q({\bf x})) = ( y - q_y({\bf x}) )^2 \, .
$$
Pour une raison évidente, cette erreur est appelée perte quadratique ou $L^2$. 

Pour simplifier les notations, nous nous plaçons dans le cas d'une classification binaire. Ce cas est aussi le plus fréquemment rencontré dans les exemples de ce cours. Lorsque la variable $y$ est binaire, la fonction s'écrit tout simplement $L(y , q({\bf x})) = ( y - q({\bf x}) )^2$, où $q({\bf x})$ s'identifie à la probabilité de la classe 1. Pour minimiser la fonction de perte quadratique, il suffit de trouver la machine probabiliste $q({\bf x})$ minimisant l'espérance conditionnelle suivante

$$
\mathbb{E}[L(y , q({\bf x})) | {\bf x} ] = \mathbb{E}[(y - q({\bf x}))^2 | {\bf x} ]  \, .
$$

Faisons apparaître la probabilité conditionnelle de la classe 1 $p(y = 1 | {\bf x})$. Nous avons 

$$
\mathbb{E}[(y - q({\bf x}))^2 | {\bf x} ]  =  \mathbb{E}[(y - p(y = 1 | {\bf x})) - (q({\bf x}) - p(y = 1 | {\bf x})) )^2 | {\bf x} ]   \, .
$$

En developpant le carré, le terme du double produit est nul. Nous avons 

$$
\mathbb{E}[(y - q({\bf x}))^2 | {\bf x} ]  =  \mathbb{E}[(y - p(y = 1 | {\bf x}))^2 | {\bf x} ]  + C({\bf x})^2  \, ,
$$
et, puisque le terme de droite est toujours positif, 

$$
\mathbb{E}[(y - q({\bf x}))^2 | {\bf x} ]  \geq  \mathbb{E}[(y - p(y = 1 | {\bf x}))^2 | {\bf x} ]  \, .
$$
Cela démontre que, pour l'erreur quadratique moyenne, la machine probabiliste optimale correspond à 

$$
q({\bf x}) =  p(y = 1 | {\bf x}) \, .
$$


Minimiser l'erreur quadratique moyenne revient donc à minimiser l'erreur log-loss. Ce résultat justifie l'utilisation fréquence de l'erreur quadratique moyenne par les algorithmes d'apprentissage. En pratique, cette erreur est calculée à partir des données empiriques de la manière suivante

$$
{\rm EQM} \approx  \frac1n \sum_{i=1}^n (y_i - q_{y_i}({\bf x}_i))^2  
$$

Lorsque la classification est binaire, cela revient à calculer

$$
{\rm EQM} \approx  \frac1n \sum_{i=1}^n (y_i - q({\bf x}_i))^2 . 
$$


## Erreur d'apprentissage et erreur de prédiction

L'apprentissage automatique concerne essentiellement des machines (ou prédicteurs) probabilistes. Elle  
consiste en la minimisation d'un critère de perte à partir d'observations répétées du couple classe/variables $(y_i,{\bf x}_i)$, $i = 1, \dots, n$, supposées indépendantes les uns des autres.   

Un algorithme d'apprentissage peut prendre des formes très diverses. De nombreux algorithmes s'appuient sur la miminisation numérique des erreurs empiriques _log loss_ ou  _quadratique_, mais ce n'est pas le cas de toutes les méthodes. En général, un algorithme correspond à la construction d'une machine probabiliste, représentée par une fonction, $\hat q({ \bf x})$, telle que

$$
\hat q({ \bf x}) = \arg\min_q \frac1n \sum_{i = 1}^n L(y_i, q({\bf x}_i)) \,.
$$

Nous appelerons parfois telle machine probabiliste un _modèle de prédiction_.


### Modèles paramétriques 

Il y a deux grandes écoles de pensée pour la construction de _modèle de prédiction_ : les approches paramétriques et les approches non-paramétriques. Les premières regroupent des approches telles que les 
modèles gaussiens, la regression logistique, les réseaux de neurones, etc. Les secondes regroupent des approches telles que les méthodes de plus proches voisins, de distances, les méthodes à noyaux, les machines à vecteurs de support, etc. Ce cours met plutôt l'accent sur les méthodes paramétriques, tout en mentionnnant les approches non-paramétriques.

Les méthodes paramétriques utilisent un ensemble de valeurs numériques, $\theta$, pour paramétrer le modèle de prédiction. Elles supposent que le modèle est une fonction des paramètres 

$$
q({\bf x}) = q({\bf x}, \theta) \,. 
$$
L'algorithme d'apprentissage consiste alors à optimiser le paramètre $\theta$. En effet la fonction d'évaluation devient-elle même une fonction de $\theta$

$$
L(\theta) = \frac1n \sum_{i = 1}^n L(y_i, q({\bf x}_i, \theta)) \,.
$$
Le problème se résume à trouver l'ensemble de paramètres $\hat \theta$ minimisant la fonction $L(\theta)$

$$
\hat \theta = \arg\min_\theta L(\theta) \,.
$$
Le _modèle de prédiction ajusté_ sera alors obtenu en injectant la valeur calculée, $\hat \theta$, dans le modèle de prédiction

$$
\hat q({\bf x}) = q({\bf x}, \hat \theta) \, .
$$ 

Si la fonction de perte et le modèle de prédiction sont suffisament réguliers (différentiables, par exemple), il est possible d'utiliser des algorithmes d'optimisation tels que l'algorithme du gradient ou la méthode de Newton pour trouver les minima de la fonction de perte. 


Pour illustrer la démarche d'optimisation, considérons les données unidimensionnelles générées par des lois exponentielles pour une classification binaire (section précédente), et définissons le modèle de prédiction suivant 

$$
q(x, \theta_1, \theta_2) = \frac{1}{ 1 + \exp(-\theta_1x-\theta_2) } \, .
$$

Pour ce problème, nous pouvons créer une fonction R calculant $L(\theta)$ en nous appuyant sur la perte logarithmique _log-loss_. Cela peut se faire  de la manière suivante 

```{r}
L <- function(th){
  # critère de décision \theta_1x + \theta_2
  z <- th[1]*x + th[2]
  # lien sigmoide 
  prob <- 1/(1 + exp(-z))
  # renvoie la log-loss
  return(- mean(y*log(prob) + (1-y)*log(1-prob)))
}
```

Il est possible de minimiser cette fonction en utilisant la commande _optim()_ de R (exemple de programmation fonctionnelle). Pour cela nous utilisons un algorithme numérique classique, appelé méthode "BFGS"

```{r}
# minimisation de la fonction L(theta)
# condition initiale th1 = -1, th2 = 2
  obj <- optim(c(-1,2), fn = L, method = "BFGS")
# solution
  obj$par
```

 Le calcul théorique de la probabilité $p(y = 1 |x)$ a été effectué dans la section précédente. Il nous indique que la solution optimale est $\theta_1 = - 0.4$ et $\theta_2 = \log(5) = 1.60$. Les valeurs numériques sont donc proches des valeurs obtenues théoriquement en considérant un échantillon de taille $n = \infty$.

### Méthodes non-paramétriques (à compléter)

Les méthodes non-paramétriques s'appuie sur un nombre très restreint de paramètres et tentent d'estimer les probabilités conditionnelles des classes par des approches locales. La méthode des _k plus proches voisins_ est une illustration directe de telles approches. 

![Méthode des k plus proches voisins : Les variables observées pour deux classes "bleues" et "rouges" correspondent aux carrés et aux triangles. On détermine la probabilité de la classe au point vert par un sondage des points voisins de ce point testé. La méthode s'avère inefficace en grande dimension à cause du fléau de la dimension.](./figures/Knn.png)

Dans la méthode des plus proches voisins, on décide d'une distance entre les variables. On détermine alors la probabilité pour qu'un vecteur ${\bf x}$ appartiennent à une classe donnée en sondant les $k$ vecteurs de l'échantillon les proches de ${\bf x}$. La méthode de sondage peut parfois introduire des pondérations des votes en fonction de la distance au vecteur cible ${\bf x}$. Cette pondération est souvent appelée noyau et plusieurs formes de noyaux peuvent être proposées.

### Malédiction de la dimension (à développer)

Exemple du cube de coté $a$ nécessaire pour occuper 10$\%$ du volume total dans le volume unité ($a \to 1$).

### Ensemble test

Les modèles paramétriques peuvent nécéssiter un grand nombre de paramètres. Nous verrons dans la suite que ce nombre peut être gigantesque pour les réseaux de neurones multicouches.

Dans ce cas, la taille de l'échantillon peut être insuffisante pour correctement évaluer la valeur moyenne de la fonction d'évaluation et le phénomène de sur-apprentissage ou sur-paramétrage (overfitting) apparait. 

![Illustration du phénomène de sur-apprentissage pour un problème de classification (rouge/bleu). La fontière noire correspond à la frontière de décision optimale. La frontière verte correspond à la frontière calculée par un modèle sur-paramétré. Source de l'image: wikipedia](./figures/Overfitting.png)



Revenons en dimension $D=1$ et supposons que l'on dispose de $n$ points répartis en $2$ classes (0 ou 1). Supposons que l'on souhaite ajuster le modèle unidimensionnel $q(x, \theta)$ décrit ci-dessous, possédant $2n$ paramètres 

$$
q(x, \theta) = \sum_{i = 1}^n \theta_{1,i} \delta_{\theta_{2,i} } (x) \,  .
$$

Pour ajuster ce modèle, nul besoin d'algorithme numérique. La solution évidente consiste à poser $\hat\theta_{1,i} = y_i$ et $\hat\theta_{2,i} = x_i$, pour tout $i$ de 1 à $n$. Les erreurs de classification, d'entropie ou de moindres carrés sont alors égales à zéro, la valeur minimale absolue.

Cette machine (inutile) montre qu'il est toujours possible de construire un algorithme obtenant une erreur nulle sur l'ensemble d'apprentissage en considèrant suffisamment de paramètres dans un modèle. Certains modèles, dits flexibles ou peu contraints, pourront montrer des problèmes similaires.

Sauf si l'ensemble d'apprentissage est infiniment grand, nous comprenons qu'il faut être vigilant lors de l'évaluation un prédicteur sur cet ensemble. 

La suite nous montre que la machine inutile n'a effectivement aucun intérêt prédictif.Nous avons vu précédement que la grandeur cible, que l'on cherche à minimiser, est l'erreur théorique suivante

$$
E = \mathbb{E}[ L(y, q(x) )] \,.
$$

Pour la machine que nous venons de construire, l'erreur est  théoriquement calculable.   Le résultat est égal à la probabilité $p(y = 1)$ pour la perte quadratique ou pour l'erreur de classification binaire. Il est égal $+\infty$ pour l'erreur d'entropie. Nous voyons que l'erreur moyenne n'est pas égale à zéro (sauf si $p(y = 0) = 1$). Si les classes sont équireprésentées, les performances théoriques de l'algorithme d'apprentissage sont très mauvaises. Nous pouvons aussi vérifier ce résultat avec une erreur calculée empiriquement sur un échantillon test indépendant de l'échantillon d'apprentissage. 

En général, on réservera un sous echantillon pour minimiser l'erreur d'apprentissage et un second sous-echantillon, composé de données non-utilisées pendant l'apprentissage, pour évaluer l'erreur cible, $E$. Selon la taille du problème, on peut se permettre de réserver un pourcentage plus ou moins élevé de données pour l'ensemble de test (20$\%$ semble être un compromis moderne acceptable). Il sera alors important de faire varier la complexité du modèle et de choisir le modèle produisant la plus petite erreur sur l'ensemble test.  

![Erreurs d'apprentissage vs erreur de test en fonction de la complexité d'un modèle. L'erreur calculée sur l'ensemble test (courbe rouge) est en général plus élevée que l'erreur calculée sur l'ensemble d'apprentissage. Le meilleur choix de modèle correspond au minimum de l'erreur calculée sur l'ensemble test. Source de l'image: wikipedia.](./figures/Overfitting2_1.png)


```{r fig.width=2, fig.height=2, echo=FALSE, fig.cap="Erreurs d'apprentissage vs erreur de test en fonction de la complexité d'un modèle. L'erreur calculée sur l'ensemble test (courbe rouge) est en général plus élevée que l'erreur calculée sur l'ensemble d'apprentissage. Le meilleur choix de modèle correspond au minimum de l'erreur calculée sur l'ensemble test. Source de l'image: wikipedia."}
#library(png)
#library(grid)
#  img <- readPNG("bookdown/figures/Overfitting3.png")
#  grid.raster(img)
```


### Exemple (séparation linéaire)

On considère un vecteur de caractéristiques ${\bf x}$ en dimension $D = 2$. On suppose que les données se répartissent en deux classes, $y = 0$ et $y = 1$. On suppose de plus que les données ont été générées en proportions égales dans chacune des deux classes.

Nous étudions un modèle génératif reposant sur des lois conditionnelles gaussiennes de moyenne respectives $\mu_0 = (1,0)$ pour la classe 0 et $\mu_1 = (0,1)$ pour la classe 1. La matrice de covariance $\Sigma$ est identique pour les deux classes et égale à 

$$
\Sigma = \left( \begin{array}{cc} 1 & 1 \\
1 & 4 \\
\end{array} \right) .
$$

Le modèle de génération de données se décrit donc de la manière suivante

$$
p( {\bf x}  | y = k ) = N( {\bf x}  | \mu_k, \Sigma ) \, , \quad  \forall k = 0, 1, \quad {\bf x} \in \mathbb{R}^2.
$$

Nous supposons les moyennes et la matrice $\Sigma$ connus (ou estimés sans erreur). Pour ce modèle génératif, comme pour l'exemple exponentiel vu précédemment, nous pouvons déterminer exactement la probabilité conditionnelle de chaque classe. le calcul conduit à

$$
p( y = 1 | {\bf x} ) = \frac{1}{1 + \exp( - {\bf w}^T  ( {\bf x} - {\bf x}_0) )}  \, ,
$$

où
$$
{\bf w} = \Sigma^{-1} (\mu_1 - \mu_0) 
$$
et
$$
{\bf x}_0 = (\mu_0 + \mu_1)/2 .
$$

Avec les valeurs choisies pour exemple, la frontière de décision optimale est la droite d'équation $x_2 = -3/2 + 5 x_1/2$. Nous pouvons visualiser la séparation des classes à l'aide d'une simulation comportant 200 données

```{r, include=FALSE}
library(magrittr)
```


```{r}
  Sigma <- c(1,1,1, 4) %>% matrix(nrow=2)
  n <- 100
  x_0 <- MASS::mvrnorm(n, mu = c(1,0), Sigma = Sigma)
  x_1 <- MASS::mvrnorm(n, mu = c(0,1), Sigma = Sigma) 
  x <- rbind(x_0, x_1)
```


```{r, include=FALSE}
  y <- rep(c("blue", "orange"), each = n)
```


```{r, echo = FALSE}
  plot(x, pch = 19, col = y)  
  abline(-3/2, 5/2, lwd = 2, col = "green3") ## trace x_2 = -3/2 + 5/2 x_1
```

Les erreurs de classification peuvent être représentée par la matrice de confusion 
```{r}
# matrice de confusion (predictions dures en ligne)
table(2*x[,2] > -3 + 5*x[,1], y == "orange")
```

L'erreur de classification est estimée de la manière suivante
```{r}
# 1 - accuracy
1 - mean( (2*x[,2] > -3 + 5*x[,1]) == (y == "orange") )
```
et l'erreur moyenne logloss est calculée de la  manière suivante
```{r}
#log-loss optimal (théorique) 
proba <- 1/(1 + exp( 5/3*x[,1] - 2/3*x[,2] - 1/2))
- mean((y=="orange")*log(proba) + (y=="bleu")*log(1-proba))
```

Envisageons maintenant l'utilisation d'un modèle paramétrique de complexité élevée. Pour cela, nous anticipons un peu le cours suivant en utilisant un réseau de neurones. Le paramètre _size_ controle la complexité du modèle, qui comportent 201 paramètres.


```{r}
mod <- nnet::nnet(x, 
                  as.numeric(y =="orange"), 
                  size = 50, 
                  maxit = 500, 
                  decay = 0.001,
                  trace = FALSE, 
                  entropy = TRUE)
```

Pour ce réseau neuronal, nous pouvons tracer la frontière de prédiction obtenue et la comparer à la frontière optimale (ligne jaune). La frontière calculée par le modèle sur-paramétré est extrêment déchiquetée, et sensible à la variance des données. Il est fort possible que les ilôts mis en evidence par la méthode neuronale soient des artifices non-reproductibles, et qu'il ne sont pas représentatifs d'un nouveau jeu de données. La frontière très irrégulière illustre le phénomène de sur-apprentissage. 


```{r, echo = FALSE}
x.coord <- seq(min(x[,1]), max(x[,1]), length = 100)
y.coord <- seq(min(x[,2]), max(x[,2]), length = 100)

matrice.test <- cbind(rep(x.coord, length = 100), 
                      rep(y.coord, each = 100))

pred <-  predict(mod, matrice.test)

#image(x.coord, y.coord,  matrix(pred > 0.5, nrow = 100), 
#      col = grey.colors(2), main = "Modèle neuronal sur-paramétré")
#plot(x, pch = 19, cex = 1.2, col = y)
#image(x.coord, y.coord,  matrix(pred, nrow = 100), 
#      col = grey.colors(2), main = "Modèle neuronal sur-paramétré")
#points(x, pch = 19, cex = 1.2, col = y)

plot(x, pch = 19, cex = 1.2, col = y, main = "Modèle neuronal sur-paramétré")
#abline(-3/2, 5/2, lwd = 4, col = "yellow") ## trace x_2 = -3/2 + 5/2 x_1
contour(x.coord,y.coord,
        matrix(pred, nrow = 100), col = "green3", 
        levels = 0.5, lwd = 2, add = T)
```

Un affichage interactif permettant de zoomer l'image nous montre les détails d'un paysage chahuté.

```{r, echo = FALSE}
library(plotly)
plot_ly(z = matrix(pred, nrow = 100), type = "contour")
```


Lorsque l'on cherche à évaluer le modèle sur l'ensemble de données d'apprentissage, les performaces sont bien supérieures à celle de la machine optimale linéaire. 

```{r}
pred <- predict(mod)
# accuracy
mean( (pred > 0.5) == (y == "orange"))
#logloss
pred[pred == 1] <-  0.999999
pred[pred == 0] <-  0.000001
- mean((y=="orange")*log(pred) + (y=="bleu")*log(1-pred))
##overfit!
```


Toutefois, en considérant un ensemble de données indépendantes, nous voyons que les performances du modèle sont nettement inférieures à celle du modèle optimal

```{r, include = FALSE}
  x.test0 <- MASS::mvrnorm(n, mu = c(1,0), Sigma = Sigma)
  x.test1 <- MASS::mvrnorm(n, mu = c(0,1), Sigma = Sigma) 
  x.test <- rbind(x.test0, x.test1)
  
  pred <- predict(mod, x.test)
```

```{r}
# accuracy
mean( (pred > 0.5) == (y == "orange"))
#logloss
pred[pred == 1] <-  0.9999999999999
pred[pred == 0] <-  0.0000000000001
- mean((y=="orange")*log(pred) + (y=="bleu")*log(1-pred))
```



